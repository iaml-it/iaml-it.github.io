<!DOCTYPE html>

<html xmlns="http://www.w3.org/1999/xhtml" lang="" xml:lang="">

<head>
  <meta charset="utf-8"/>
  <meta name="viewport" content="width=device-width, initial-scale=1"/>
  <meta http-equiv="X-UA-Compatible" content="IE=Edge,chrome=1"/>
  <meta name="generator" content="distill" />

  <style type="text/css">
  /* Hide doc at startup (prevent jankiness while JS renders/transforms) */
  body {
    visibility: hidden;
  }
  </style>

 <!--radix_placeholder_import_source-->
 <!--/radix_placeholder_import_source-->

<style type="text/css">code{white-space: pre;}</style>
<style type="text/css" data-origin="pandoc">
pre > code.sourceCode { white-space: pre; position: relative; }
pre > code.sourceCode > span { display: inline-block; line-height: 1.25; }
pre > code.sourceCode > span:empty { height: 1.2em; }
.sourceCode { overflow: visible; }
code.sourceCode > span { color: inherit; text-decoration: inherit; }
div.sourceCode { margin: 1em 0; }
pre.sourceCode { margin: 0; }
@media screen {
div.sourceCode { overflow: auto; }
}
@media print {
pre > code.sourceCode { white-space: pre-wrap; }
pre > code.sourceCode > span { text-indent: -5em; padding-left: 5em; }
}
pre.numberSource code
  { counter-reset: source-line 0; }
pre.numberSource code > span
  { position: relative; left: -4em; counter-increment: source-line; }
pre.numberSource code > span > a:first-child::before
  { content: counter(source-line);
    position: relative; left: -1em; text-align: right; vertical-align: baseline;
    border: none; display: inline-block;
    -webkit-touch-callout: none; -webkit-user-select: none;
    -khtml-user-select: none; -moz-user-select: none;
    -ms-user-select: none; user-select: none;
    padding: 0 4px; width: 4em;
    color: #aaaaaa;
  }
pre.numberSource { margin-left: 3em; border-left: 1px solid #aaaaaa;  padding-left: 4px; }
div.sourceCode
  {   }
@media screen {
pre > code.sourceCode > span > a:first-child::before { text-decoration: underline; }
}
code span.al { color: #ad0000; } /* Alert */
code span.an { color: #5e5e5e; } /* Annotation */
code span.at { color: #20794d; } /* Attribute */
code span.bn { color: #ad0000; } /* BaseN */
code span.bu { } /* BuiltIn */
code span.cf { color: #007ba5; } /* ControlFlow */
code span.ch { color: #20794d; } /* Char */
code span.cn { color: #8f5902; } /* Constant */
code span.co { color: #5e5e5e; } /* Comment */
code span.cv { color: #5e5e5e; font-style: italic; } /* CommentVar */
code span.do { color: #5e5e5e; font-style: italic; } /* Documentation */
code span.dt { color: #ad0000; } /* DataType */
code span.dv { color: #ad0000; } /* DecVal */
code span.er { color: #ad0000; } /* Error */
code span.ex { } /* Extension */
code span.fl { color: #ad0000; } /* Float */
code span.fu { color: #4758ab; } /* Function */
code span.im { } /* Import */
code span.in { color: #5e5e5e; } /* Information */
code span.kw { color: #007ba5; } /* Keyword */
code span.op { color: #5e5e5e; } /* Operator */
code span.ot { color: #007ba5; } /* Other */
code span.pp { color: #ad0000; } /* Preprocessor */
code span.sc { color: #20794d; } /* SpecialChar */
code span.ss { color: #20794d; } /* SpecialString */
code span.st { color: #20794d; } /* String */
code span.va { color: #111111; } /* Variable */
code span.vs { color: #20794d; } /* VerbatimString */
code span.wa { color: #5e5e5e; font-style: italic; } /* Warning */
</style>

  <!--radix_placeholder_meta_tags-->
<title>IAML Distill Blog: Transformers in Vision</title>

<meta property="description" itemprop="description" content="What have Vision Transformers been up to?"/>

<link rel="canonical" href="https://iaml-it.github.io/distill/posts/2021-04-28-transformers-in-vision/"/>
<link rel="license" href="https://creativecommons.org/licenses/by/4.0/"/>

<!--  https://schema.org/Article -->
<meta property="article:published" itemprop="datePublished" content="2021-04-28"/>
<meta property="article:created" itemprop="dateCreated" content="2021-04-28"/>
<meta name="article:author" content="Niccolò Zanichelli"/>

<!--  https://developers.facebook.com/docs/sharing/webmasters#markup -->
<meta property="og:title" content="IAML Distill Blog: Transformers in Vision"/>
<meta property="og:type" content="article"/>
<meta property="og:description" content="What have Vision Transformers been up to?"/>
<meta property="og:url" content="https://iaml-it.github.io/distill/posts/2021-04-28-transformers-in-vision/"/>
<meta property="og:image" content="https://iaml-it.github.io/distill/posts/2021-04-28-transformers-in-vision/deit.png"/>
<meta property="og:image:width" content="842"/>
<meta property="og:image:height" content="801"/>
<meta property="og:locale" content="en_US"/>
<meta property="og:site_name" content="IAML Distill Blog"/>

<!--  https://dev.twitter.com/cards/types/summary -->
<meta property="twitter:card" content="summary_large_image"/>
<meta property="twitter:title" content="IAML Distill Blog: Transformers in Vision"/>
<meta property="twitter:description" content="What have Vision Transformers been up to?"/>
<meta property="twitter:url" content="https://iaml-it.github.io/distill/posts/2021-04-28-transformers-in-vision/"/>
<meta property="twitter:image" content="https://iaml-it.github.io/distill/posts/2021-04-28-transformers-in-vision/deit.png"/>
<meta property="twitter:image:width" content="842"/>
<meta property="twitter:image:height" content="801"/>
<meta property="twitter:site" content="@iaml_it"/>
<meta property="twitter:creator" content="@nickz_42"/>

<!--  https://scholar.google.com/intl/en/scholar/inclusion.html#indexing -->
<meta name="citation_title" content="IAML Distill Blog: Transformers in Vision"/>
<meta name="citation_fulltext_html_url" content="https://iaml-it.github.io/distill/posts/2021-04-28-transformers-in-vision/"/>
<meta name="citation_fulltext_world_readable" content=""/>
<meta name="citation_online_date" content="2021/04/28"/>
<meta name="citation_publication_date" content="2021/04/28"/>
<meta name="citation_author" content="Niccolò Zanichelli"/>
<meta name="citation_author_institution" content="University of Parma"/>
<!--/radix_placeholder_meta_tags-->
  
  <meta name="citation_reference" content="citation_title=Attention is all you need;citation_publication_date=2017;citation_author=Ashish Vaswani;citation_author=Noam Shazeer;citation_author=Niki Parmar;citation_author=Jakob Uszkoreit;citation_author=Llion Jones;citation_author=Aidan N. Gomez;citation_author=Lukasz Kaiser;citation_author=Illia Polosukhin"/>
  <meta name="citation_reference" content="citation_title=BERT: Pre-training of deep bidirectional transformers for language understanding;citation_publication_date=2019;citation_author=Jacob Devlin;citation_author=Ming-Wei Chang;citation_author=Kenton Lee;citation_author=Kristina Toutanova"/>
  <meta name="citation_reference" content="citation_title=Megatron-LM: Training multi-billion parameter language models using model parallelism;citation_publication_date=2019;citation_volume=abs/1909.08053;citation_author=Mohammad Shoeybi;citation_author=Mostofa Patwary;citation_author=Raul Puri;citation_author=Patrick LeGresley;citation_author=Jared Casper;citation_author=Bryan Catanzaro"/>
  <meta name="citation_reference" content="citation_title=Exploring the limits of transfer learning with a unified text-to-text transformer;citation_publication_date=2020;citation_volume=21;citation_author=Colin Raffel;citation_author=Noam Shazeer;citation_author=Adam Roberts;citation_author=Katherine Lee;citation_author=Sharan Narang;citation_author=Michael Matena;citation_author=Yanqi Zhou;citation_author=Wei Li;citation_author=Peter J. Liu"/>
  <meta name="citation_reference" content="citation_title=Improving language understanding with unsupervised learning;citation_publication_date=2019;citation_publisher=OpenAI;citation_author=Alec Radford"/>
  <meta name="citation_reference" content="citation_title=Better language models and their implications;citation_publication_date=2018;citation_publisher=OpenAI;citation_author=Alec Radford"/>
  <meta name="citation_reference" content="citation_title=Language models are few-shot learners;citation_publication_date=2020;citation_author=Tom B. Brown;citation_author=Benjamin Mann;citation_author=Nick Ryder;citation_author=Melanie Subbiah;citation_author=Jared Kaplan;citation_author=Prafulla Dhariwal;citation_author=Arvind Neelakantan;citation_author=Pranav Shyam;citation_author=Girish Sastry;citation_author=Amanda Askell;citation_author=Sandhini Agarwal;citation_author=Ariel Herbert-Voss;citation_author=Gretchen Krueger;citation_author=Tom Henighan;citation_author=Rewon Child;citation_author=Aditya Ramesh;citation_author=Daniel M. Ziegler;citation_author=Jeffrey Wu;citation_author=Clemens Winter;citation_author=Christopher Hesse;citation_author=Mark Chen;citation_author=Eric Sigler;citation_author=Mateusz Litwin;citation_author=Scott Gray;citation_author=Benjamin Chess;citation_author=Jack Clark;citation_author=Christopher Berner;citation_author=Sam McCandlish;citation_author=Alec Radford;citation_author=Ilya Sutskever;citation_author=Dario Amodei"/>
  <meta name="citation_reference" content="citation_title=Attention augmented convolutional networks;citation_publication_date=2019;citation_publisher=IEEE;citation_doi=10.1109/ICCV.2019.00338;citation_author=Irwan Bello;citation_author=Barret Zoph;citation_author=Quoc Le;citation_author=Ashish Vaswani;citation_author=Jonathon Shlens"/>
  <meta name="citation_reference" content="citation_title=Stand-alone self-attention in vision models;citation_publication_date=2019;citation_author=Niki Parmar;citation_author=Prajit Ramachandran;citation_author=Ashish Vaswani;citation_author=Irwan Bello;citation_author=Anselm Levskaya;citation_author=Jon Shlens"/>
  <meta name="citation_reference" content="citation_title=End-to-end object detection with transformers;citation_publication_date=2020;citation_publisher=Springer;citation_volume=12346;citation_doi=10.1007/978-3-030-58452-8\_13;citation_author=Nicolas Carion;citation_author=Francisco Massa;citation_author=Gabriel Synnaeve;citation_author=Nicolas Usunier;citation_author=Alexander Kirillov;citation_author=Sergey Zagoruyko"/>
  <meta name="citation_reference" content="citation_title=Visual transformers: Token-based image representation and processing for computer vision;citation_publication_date=2020;citation_volume=abs/2006.03677;citation_author=Bichen Wu;citation_author=Chenfeng Xu;citation_author=Xiaoliang Dai;citation_author=Alvin Wan;citation_author=Peizhao Zhang;citation_author=Masayoshi Tomizuka;citation_author=Kurt Keutzer;citation_author=Peter Vajda"/>
  <meta name="citation_reference" content="citation_title=LambdaNetworks: Modeling long-range interactions without attention;citation_publication_date=2021;citation_volume=abs/2102.08602;citation_author=Irwan Bello"/>
  <meta name="citation_reference" content="citation_title=Image transformer;citation_publication_date=2018;citation_publisher=PMLR;citation_volume=80;citation_author=Niki Parmar;citation_author=Ashish Vaswani;citation_author=Jakob Uszkoreit;citation_author=Lukasz Kaiser;citation_author=Noam Shazeer;citation_author=Alexander Ku;citation_author=Dustin Tran"/>
  <meta name="citation_reference" content="citation_title=Axial attention in multidimensional transformers;citation_publication_date=2019;citation_volume=abs/1912.12180;citation_author=Jonathan Ho;citation_author=Nal Kalchbrenner;citation_author=Dirk Weissenborn;citation_author=Tim Salimans"/>
  <meta name="citation_reference" content="citation_title=A survey on visual transformer;citation_publication_date=2020;citation_volume=abs/2012.12556;citation_author=Kai Han;citation_author=Yunhe Wang;citation_author=Hanting Chen;citation_author=Xinghao Chen;citation_author=Jianyuan Guo;citation_author=Zhenhua Liu;citation_author=Yehui Tang;citation_author=An Xiao;citation_author=Chunjing Xu;citation_author=Yixing Xu;citation_author=Zhaohui Yang;citation_author=Yiman Zhang;citation_author=Dacheng Tao"/>
  <meta name="citation_reference" content="citation_title=Transformers in vision: A survey;citation_publication_date=2021;citation_volume=abs/2101.01169;citation_author=Salman Khan;citation_author=Muzammal Naseer;citation_author=Munawar Hayat;citation_author=Syed Waqas Zamir;citation_author=Fahad Shahbaz Khan;citation_author=Mubarak Shah"/>
  <meta name="citation_reference" content="citation_title=An image is worth 16x16 words: Transformers for image recognition at scale;citation_publication_date=2021;citation_author=Alexey Dosovitskiy;citation_author=Lucas Beyer;citation_author=Alexander Kolesnikov;citation_author=Dirk Weissenborn;citation_author=Xiaohua Zhai;citation_author=Thomas Unterthiner;citation_author=Mostafa Dehghani;citation_author=Matthias Minderer;citation_author=Georg Heigold;citation_author=Sylvain Gelly;citation_author=Jakob Uszkoreit;citation_author=Neil Houlsby"/>
  <meta name="citation_reference" content="citation_title=Training data-efficient image transformers &amp; distillation through attention;citation_publication_date=2020;citation_volume=abs/2012.12877;citation_author=Hugo Touvron;citation_author=Matthieu Cord;citation_author=Matthijs Douze;citation_author=Francisco Massa;citation_author=Alexandre Sablayrolles;citation_author=Hervé Jégou"/>
  <meta name="citation_reference" content="citation_title=Deep networks with stochastic depth;citation_publication_date=2016;citation_publisher=Springer;citation_volume=9908;citation_doi=10.1007/978-3-319-46493-0\_39;citation_author=Gao Huang;citation_author=Yu Sun;citation_author=Zhuang Liu;citation_author=Daniel Sedra;citation_author=Kilian Q. Weinberger"/>
  <meta name="citation_reference" content="citation_title=RandAugment: Practical automated data augmentation with a reduced search space;citation_publication_date=2020;citation_author=Ekin Dogus Cubuk;citation_author=Barret Zoph;citation_author=Jon Shlens;citation_author=Quoc Le"/>
  <meta name="citation_reference" content="citation_title=Mixup: Beyond empirical risk minimization;citation_publication_date=2018;citation_publisher=OpenReview.net;citation_author=Hongyi Zhang;citation_author=Moustapha Cissé;citation_author=Yann N. Dauphin;citation_author=David Lopez-Paz"/>
  <meta name="citation_reference" content="citation_title=CutMix: Regularization strategy to train strong classifiers with localizable features;citation_publication_date=2019;citation_publisher=IEEE;citation_doi=10.1109/ICCV.2019.00612;citation_author=Sangdoo Yun;citation_author=Dongyoon Han;citation_author=Sanghyuk Chun;citation_author=Seong Joon Oh;citation_author=Youngjoon Yoo;citation_author=Junsuk Choe"/>
  <meta name="citation_reference" content="citation_title=Random erasing data augmentation;citation_publication_date=2020;citation_publisher=AAAI Press;citation_author=Zhun Zhong;citation_author=Liang Zheng;citation_author=Guoliang Kang;citation_author=Shaozi Li;citation_author=Yi Yang"/>
  <meta name="citation_reference" content="citation_title=Dropout: A simple way to prevent neural networks from overfitting;citation_publication_date=2014;citation_volume=15;citation_author=Nitish Srivastava;citation_author=Geoffrey E. Hinton;citation_author=Alex Krizhevsky;citation_author=Ilya Sutskever;citation_author=Ruslan Salakhutdinov"/>
  <meta name="citation_reference" content="citation_title=Bottleneck transformers for visual recognition;citation_publication_date=2021;citation_volume=abs/2101.11605;citation_author=Aravind Srinivas;citation_author=Tsung-Yi Lin;citation_author=Niki Parmar;citation_author=Jonathon Shlens;citation_author=Pieter Abbeel;citation_author=Ashish Vaswani"/>
  <meta name="citation_reference" content="citation_title=Squeeze-and-excitation networks;citation_publication_date=2020;citation_volume=42;citation_doi=10.1109/TPAMI.2019.2913372;citation_author=Jie Hu;citation_author=Li Shen;citation_author=Samuel Albanie;citation_author=Gang Sun;citation_author=Enhua Wu"/>
  <meta name="citation_reference" content="citation_title=Sigmoid-weighted linear units for neural network function approximation in reinforcement learning;citation_publication_date=2018;citation_volume=107;citation_doi=10.1016/j.neunet.2017.12.012;citation_author=Stefan Elfwing;citation_author=Eiji Uchibe;citation_author=Kenji Doya"/>
  <meta name="citation_reference" content="citation_title=EfficientNet: Rethinking model scaling for convolutional neural networks;citation_publication_date=2019;citation_publisher=PMLR;citation_volume=97;citation_author=Mingxing Tan;citation_author=Quoc V. Le"/>
  <meta name="citation_reference" content="citation_title=Conditional positional encodings for vision transformers;citation_publication_date=2021;citation_volume=abs/2102.10882;citation_author=Xiangxiang Chu;citation_author=Zhi Tian;citation_author=Bo Zhang;citation_author=Xinlong Wang;citation_author=Xiaolin Wei;citation_author=Huaxia Xia;citation_author=Chunhua Shen"/>
  <meta name="citation_reference" content="citation_title=Transformer in transformer;citation_publication_date=2021;citation_volume=abs/2103.00112;citation_author=Kai Han;citation_author=An Xiao;citation_author=Enhua Wu;citation_author=Jianyuan Guo;citation_author=Chunjing Xu;citation_author=Yunhe Wang"/>
  <meta name="citation_reference" content="citation_title=DeepViT: Towards deeper vision transformer;citation_publication_date=2021;citation_volume=abs/2103.11886;citation_author=Daquan Zhou;citation_author=Bingyi Kang;citation_author=Xiaojie Jin;citation_author=Linjie Yang;citation_author=Xiaochen Lian;citation_author=Qibin Hou;citation_author=Jiashi Feng"/>
  <meta name="citation_reference" content="citation_title=LazyFormer: Self attention with lazy update;citation_publication_date=2021;citation_volume=abs/2102.12702;citation_author=Chengxuan Ying;citation_author=Guolin Ke;citation_author=Di He;citation_author=Tie-Yan Liu"/>
  <meta name="citation_reference" content="citation_title=Going deeper with image transformers;citation_publication_date=2021;citation_volume=abs/2103.17239;citation_author=Hugo Touvron;citation_author=Matthieu Cord;citation_author=Alexandre Sablayrolles;citation_author=Gabriel Synnaeve;citation_author=Hervé Jégou"/>
  <meta name="citation_reference" content="citation_title=Fixup initialization: Residual learning without normalization;citation_publication_date=2019;citation_publisher=OpenReview.net;citation_author=Hongyi Zhang;citation_author=Yann N. Dauphin;citation_author=Tengyu Ma"/>
  <meta name="citation_reference" content="citation_title=Batch normalization biases residual blocks towards the identity function in deep networks;citation_publication_date=2020;citation_author=Soham De;citation_author=Samuel L. Smith"/>
  <meta name="citation_reference" content="citation_title=Talking-heads attention;citation_publication_date=2020;citation_volume=abs/2003.02436;citation_author=Noam Shazeer;citation_author=Zhenzhong Lan;citation_author=Youlong Cheng;citation_author=Nan Ding;citation_author=Le Hou"/>
  <meta name="citation_reference" content="citation_title=High-performance large-scale image recognition without normalization;citation_publication_date=2021;citation_volume=abs/2102.06171;citation_author=Andrew Brock;citation_author=Soham De;citation_author=Samuel L. Smith;citation_author=Karen Simonyan"/>
  <meta name="citation_reference" content="citation_title=ConViT: Improving vision transformers with soft convolutional inductive biases;citation_publication_date=2021;citation_volume=abs/2103.10697;citation_author=Stéphane Ascoli;citation_author=Hugo Touvron;citation_author=Matthew L. Leavitt;citation_author=Ari S. Morcos;citation_author=Giulio Biroli;citation_author=Levent Sagun"/>
  <meta name="citation_reference" content="citation_title=On the relationship between self-attention and convolutional layers;citation_publication_date=2020;citation_publisher=OpenReview.net;citation_author=Jean-Baptiste Cordonnier;citation_author=Andreas Loukas;citation_author=Martin Jaggi"/>
  <meta name="citation_reference" content="citation_title=Incorporating convolution designs into visual transformers;citation_publication_date=2021;citation_volume=abs/2103.11816;citation_author=Kun Yuan;citation_author=Shaopeng Guo;citation_author=Ziwei Liu;citation_author=Aojun Zhou;citation_author=Fengwei Yu;citation_author=Wei Wu"/>
  <meta name="citation_reference" content="citation_title=LocalViT: Bringing locality to vision transformers;citation_publication_date=2021;citation_volume=abs/2104.05707;citation_author=Yawei Li;citation_author=Kai Zhang;citation_author=Jiezhang Cao;citation_author=Radu Timofte;citation_author=Luc Van Gool"/>
  <meta name="citation_reference" content="citation_title=Tokens-to-token ViT: Training vision transformers from scratch on ImageNet;citation_publication_date=2021;citation_volume=abs/2101.11986;citation_author=Li Yuan;citation_author=Yunpeng Chen;citation_author=Tao Wang;citation_author=Weihao Yu;citation_author=Yujun Shi;citation_author=Francis E. H. Tay;citation_author=Jiashi Feng;citation_author=Shuicheng Yan"/>
  <meta name="citation_reference" content="citation_title=MobileNetV2: Inverted residuals and linear bottlenecks;citation_publication_date=2018;citation_publisher=IEEE Computer Society;citation_doi=10.1109/CVPR.2018.00474;citation_author=Mark Sandler;citation_author=Andrew G. Howard;citation_author=Menglong Zhu;citation_author=Andrey Zhmoginov;citation_author=Liang-Chieh Chen"/>
  <meta name="citation_reference" content="citation_title=Pyramid vision transformer: A versatile backbone for dense prediction without convolutions;citation_publication_date=2021;citation_volume=abs/2102.12122;citation_author=Wenhai Wang;citation_author=Enze Xie;citation_author=Xiang Li;citation_author=Deng-Ping Fan;citation_author=Kaitao Song;citation_author=Ding Liang;citation_author=Tong Lu;citation_author=Ping Luo;citation_author=Ling Shao"/>
  <meta name="citation_reference" content="citation_title=Scalable visual transformers with hierarchical pooling;citation_publication_date=2021;citation_volume=abs/2103.10619;citation_author=Zizheng Pan;citation_author=Bohan Zhuang;citation_author=Jing Liu;citation_author=Haoyu He;citation_author=Jianfei Cai"/>
  <meta name="citation_reference" content="citation_title=Swin transformer: Hierarchical vision transformer using shifted windows;citation_publication_date=2021;citation_volume=abs/2103.14030;citation_author=Ze Liu;citation_author=Yutong Lin;citation_author=Yue Cao;citation_author=Han Hu;citation_author=Yixuan Wei;citation_author=Zheng Zhang;citation_author=Stephen Lin;citation_author=Baining Guo"/>
  <meta name="citation_reference" content="citation_title=Rethinking spatial dimensions of vision transformers;citation_publication_date=2021;citation_volume=abs/2103.16302;citation_author=Byeongho Heo;citation_author=Sangdoo Yun;citation_author=Dongyoon Han;citation_author=Sanghyuk Chun;citation_author=Junsuk Choe;citation_author=Seong Joon Oh"/>
  <meta name="citation_reference" content="citation_title=LeViT: A vision transformer in ConvNet’s clothing for faster inference;citation_publication_date=2021;citation_volume=abs/2104.01136;citation_author=Benjamin Graham;citation_author=Alaaeldin El-Nouby;citation_author=Hugo Touvron;citation_author=Pierre Stock;citation_author=Armand Joulin;citation_author=Hervé Jégou;citation_author=Matthijs Douze"/>
  <meta name="citation_reference" content="citation_title=Bridging nonlinearities and stochastic regularizers with gaussian error linear units;citation_publication_date=2016;citation_volume=abs/1606.08415;citation_author=Dan Hendrycks;citation_author=Kevin Gimpel"/>
  <meta name="citation_reference" content="citation_title=CrossViT: Cross-attention multi-scale vision transformer for image classification;citation_publication_date=2021;citation_volume=abs/2103.14899;citation_author=Chun-Fu Chen;citation_author=Quanfu Fan;citation_author=Rameswar Panda"/>
  <meta name="citation_reference" content="citation_title=CvT: Introducing convolutions to vision transformers;citation_publication_date=2021;citation_volume=abs/2103.15808;citation_author=Haiping Wu;citation_author=Bin Xiao;citation_author=Noel Codella;citation_author=Mengchen Liu;citation_author=Xiyang Dai;citation_author=Lu Yuan;citation_author=Lei Zhang"/>
  <meta name="citation_reference" content="citation_title=Multi-scale vision longformer: A new vision transformer for high-resolution image encoding;citation_publication_date=2021;citation_volume=abs/2103.15358;citation_author=Pengchuan Zhang;citation_author=Xiyang Dai;citation_author=Jianwei Yang;citation_author=Bin Xiao;citation_author=Lu Yuan;citation_author=Lei Zhang;citation_author=Jianfeng Gao"/>
  <meta name="citation_reference" content="citation_title=Xception: Deep learning with depthwise separable convolutions;citation_publication_date=2017;citation_publisher=IEEE Computer Society;citation_doi=10.1109/CVPR.2017.195;citation_author=François Chollet"/>
  <meta name="citation_reference" content="citation_title=Longformer: The long-document transformer;citation_publication_date=2020;citation_volume=abs/2004.05150;citation_author=Iz Beltagy;citation_author=Matthew E. Peters;citation_author=Arman Cohan"/>
  <meta name="citation_reference" content="citation_title=Scaling local self-attention for parameter efficient visual backbones;citation_publication_date=2021;citation_volume=abs/2103.12731;citation_author=Ashish Vaswani;citation_author=Prajit Ramachandran;citation_author=Aravind Srinivas;citation_author=Niki Parmar;citation_author=Blake A. Hechtman;citation_author=Jonathon Shlens"/>
  <meta name="citation_reference" content="citation_title=So-ViT: Mind visual tokens for vision transformer;citation_publication_date=2021;citation_volume=abs/2104.10935;citation_author=Jiangtao Xie;citation_author=Ruiren Zeng;citation_author=Qilong Wang;citation_author=Ziqi Zhou;citation_author=Peihua Li"/>
  <meta name="citation_reference" content="citation_title=An analysis of deep neural network models for practical applications;citation_publication_date=2016;citation_volume=abs/1605.07678;citation_author=Alfredo Canziani;citation_author=Adam Paszke;citation_author=Eugenio Culurciello"/>
  <meta name="citation_reference" content="citation_title=Understanding robustness of transformers for image classification;citation_publication_date=2021;citation_volume=abs/2103.14586;citation_author=Srinadh Bhojanapalli;citation_author=Ayan Chakrabarti;citation_author=Daniel Glasner;citation_author=Daliang Li;citation_author=Thomas Unterthiner;citation_author=Andreas Veit"/>
  <meta name="citation_reference" content="citation_title=Group equivariant stand-alone self-attention for vision;citation_publication_date=2021;citation_author=David W. Romero;citation_author=Jean-Baptiste Cordonnier"/>
  <meta name="citation_reference" content="citation_title=Generative pretraining from pixels;citation_publication_date=2020;citation_author=Mark Chen;citation_author=Alec Radford;citation_author=Rewon Child;citation_author=Jeff Wu;citation_author=Heewoo Jun;citation_author=David Luan;citation_author=Ilya Sutskever"/>
  <meta name="citation_reference" content="citation_title=SiT: Self-supervised vIsion transformer;citation_publication_date=2021;citation_volume=abs/2104.03602;citation_author=Sara Atito Ali Ahmed;citation_author=Muhammad Awais;citation_author=Josef Kittler"/>
  <meta name="citation_reference" content="citation_title=TransUNet: Transformers make strong encoders for medical image segmentation;citation_publication_date=2021;citation_volume=abs/2102.04306;citation_author=Jieneng Chen;citation_author=Yongyi Lu;citation_author=Qihang Yu;citation_author=Xiangde Luo;citation_author=Ehsan Adeli;citation_author=Yan Wang;citation_author=Le Lu;citation_author=Alan L. Yuille;citation_author=Yuyin Zhou"/>
  <meta name="citation_reference" content="citation_title=Medical transformer: Gated axial-attention for medical image segmentation;citation_publication_date=2021;citation_volume=abs/2102.10662;citation_author=Jeya Maria Jose Valanarasu;citation_author=Poojan Oza;citation_author=Ilker Hacihaliloglu;citation_author=Vishal M. Patel"/>
  <meta name="citation_reference" content="citation_title=UNETR: Transformers for 3D medical image segmentation;citation_publication_date=2021;citation_volume=abs/2103.10504;citation_author=Ali Hatamizadeh;citation_author=Dong Yang;citation_author=Holger Roth;citation_author=Daguang Xu"/>
  <meta name="citation_reference" content="citation_title=Video transformer network;citation_publication_date=2021;citation_volume=abs/2102.00719;citation_author=Daniel Neimark;citation_author=Omri Bar;citation_author=Maya Zohar;citation_author=Dotan Asselmann"/>
  <meta name="citation_reference" content="citation_title=Is space-time attention all you need for video understanding?;citation_publication_date=2021;citation_volume=abs/2102.05095;citation_author=Gedas Bertasius;citation_author=Heng Wang;citation_author=Lorenzo Torresani"/>
  <meta name="citation_reference" content="citation_title=Multiscale vision transformers;citation_publication_date=2021;citation_volume=abs/2104.11227;citation_author=Haoqi Fan;citation_author=Bo Xiong;citation_author=Karttikeya Mangalam;citation_author=Yanghao Li;citation_author=Zhicheng Yan;citation_author=Jitendra Malik;citation_author=Christoph Feichtenhofer"/>
  <meta name="citation_reference" content="citation_title=ViViT: A video vision transformer;citation_publication_date=2021;citation_volume=abs/2103.15691;citation_author=Anurag Arnab;citation_author=Mostafa Dehghani;citation_author=Georg Heigold;citation_author=Chen Sun;citation_author=Mario Lucic;citation_author=Cordelia Schmid"/>
  <meta name="citation_reference" content="citation_title=SlowFast networks for video recognition;citation_publication_date=2019;citation_publisher=IEEE;citation_doi=10.1109/ICCV.2019.00630;citation_author=Christoph Feichtenhofer;citation_author=Haoqi Fan;citation_author=Jitendra Malik;citation_author=Kaiming He"/>
  <meta name="citation_reference" content="citation_title=The kinetics human action video dataset;citation_publication_date=2017;citation_volume=abs/1705.06950;citation_author=Will Kay;citation_author=João Carreira;citation_author=Karen Simonyan;citation_author=Brian Zhang;citation_author=Chloe Hillier;citation_author=Sudheendra Vijayanarasimhan;citation_author=Fabio Viola;citation_author=Tim Green;citation_author=Trevor Back;citation_author=Paul Natsev;citation_author=Mustafa Suleyman;citation_author=Andrew Zisserman"/>
  <meta name="citation_reference" content="citation_title=An image is worth 16x16 words, what is a video worth?;citation_publication_date=2021;citation_volume=abs/2103.13915;citation_author=Gilad Sharir;citation_author=Asaf Noy;citation_author=Lihi Zelnik-Manor"/>
  <meta name="citation_reference" content="citation_title=X3D: Expanding architectures for efficient video recognition;citation_publication_date=2020;citation_publisher=IEEE;citation_doi=10.1109/CVPR42600.2020.00028;citation_author=Christoph Feichtenhofer"/>
  <meta name="citation_reference" content="citation_title=EfficientNetV2: Smaller models and faster training;citation_publication_date=2021;citation_volume=abs/2104.00298;citation_author=Mingxing Tan;citation_author=Quoc V. Le"/>
  <meta name="citation_reference" content="citation_title=AST: Audio spectrogram transformer;citation_publication_date=2021;citation_volume=abs/2104.01778;citation_author=Yuan Gong;citation_author=Yu-An Chung;citation_author=James R. Glass"/>
  <meta name="citation_reference" content="citation_title=Keyword transformer: A self-attention model for keyword spotting;citation_publication_date=2021;citation_volume=abs/2104.00769;citation_author=Axel Berg;citation_author=Mark O’Connor;citation_author=Miguel Tairum Cruz"/>
  <meta name="citation_reference" content="citation_title=ViLT: Vision-and-language transformer without convolution or region supervision;citation_publication_date=2021;citation_volume=abs/2102.03334;citation_author=Wonjae Kim;citation_author=Bokyung Son;citation_author=Ildoo Kim"/>
  <meta name="citation_reference" content="citation_title=VATT: Transformers for multimodal self-supervised learning from raw video, audio and text;citation_publication_date=2021;citation_volume=abs/2104.11178;citation_author=Hassan Akbari;citation_author=Liangzhe Yuan;citation_author=Rui Qian;citation_author=Wei-Hong Chuang;citation_author=Shih-Fu Chang;citation_author=Yin Cui;citation_author=Boqing Gong"/>
  <meta name="citation_reference" content="citation_title=Perceiver: General perception with iterative attention;citation_publication_date=2021;citation_volume=abs/2103.03206;citation_author=Andrew Jaegle;citation_author=Felix Gimeno;citation_author=Andrew Brock;citation_author=Andrew Zisserman;citation_author=Oriol Vinyals;citation_author=João Carreira"/>
  <meta name="citation_reference" content="citation_title=An empirical study of training self-supervised vision transformers;citation_publication_date=2021;citation_volume=abs/2104.02057;citation_author=Xinlei Chen;citation_author=Saining Xie;citation_author=Kaiming He"/>
  <meta name="citation_reference" content="citation_title=Revisiting ResNets: Improved training and scaling strategies;citation_publication_date=2021;citation_volume=abs/2103.07579;citation_author=Irwan Bello;citation_author=William Fedus;citation_author=Xianzhi Du;citation_author=Ekin D. Cubuk;citation_author=Aravind Srinivas;citation_author=Tsung-Yi Lin;citation_author=Jonathon Shlens;citation_author=Barret Zoph"/>
  <!--radix_placeholder_rmarkdown_metadata-->

<script type="text/json" id="radix-rmarkdown-metadata">
{"type":"list","attributes":{"names":{"type":"character","attributes":{},"value":["title","description","base_url","preview","author","date","output","creative_commons","bibliography","csl","link-citations","twitter","citation_url","canonical_url"]}},"value":[{"type":"character","attributes":{},"value":["Transformers in Vision"]},{"type":"character","attributes":{},"value":["What have Vision Transformers been up to?"]},{"type":"character","attributes":{},"value":["https://iaml-it.github.io/"]},{"type":"character","attributes":{},"value":["posts/2021-04-28-transformers-in-vision/imagenet.png"]},{"type":"list","attributes":{},"value":[{"type":"list","attributes":{"names":{"type":"character","attributes":{},"value":["first_name","last_name","url","affiliation"]}},"value":[{"type":"character","attributes":{},"value":["Niccolò"]},{"type":"character","attributes":{},"value":["Zanichelli"]},{"type":"character","attributes":{},"value":["https://twitter.com/nickz_42"]},{"type":"character","attributes":{},"value":["University of Parma"]}]}]},{"type":"character","attributes":{},"value":["04-28-2021"]},{"type":"list","attributes":{"names":{"type":"character","attributes":{},"value":["distill::distill_article"]}},"value":[{"type":"list","attributes":{"names":{"type":"character","attributes":{},"value":["toc","toc_depth","self_contained"]}},"value":[{"type":"logical","attributes":{},"value":[true]},{"type":"integer","attributes":{},"value":[2]},{"type":"logical","attributes":{},"value":[false]}]}]},{"type":"character","attributes":{},"value":["CC BY"]},{"type":"character","attributes":{},"value":["bibliography.bib"]},{"type":"character","attributes":{},"value":["nature.csl"]},{"type":"logical","attributes":{},"value":[true]},{"type":"list","attributes":{"names":{"type":"character","attributes":{},"value":["site","creator"]}},"value":[{"type":"character","attributes":{},"value":["@iaml_it"]},{"type":"character","attributes":{},"value":["@nickz_42"]}]},{"type":"character","attributes":{},"value":["https://iaml-it.github.io/distill/posts/2021-04-28-transformers-in-vision/"]},{"type":"character","attributes":{},"value":["https://iaml-it.github.io/distill/posts/2021-04-28-transformers-in-vision/"]}]}
</script>
<!--/radix_placeholder_rmarkdown_metadata-->
  
  <script type="text/json" id="radix-resource-manifest">
  {"type":"character","attributes":{},"value":["bibliography.bib","cait.png","conditional.png","convit.png","crossvit.png","cvt.png","deepvit.png","deit.png","halonet.png","iaml-distill-blog.csl","imagenet.png","imagenet1k.html","imagenet1k.png","imagenet21k.html","medt.png","nature.csl","pvt.png","random.png","sit.png","sovit.png","swin1.png","swin2.png","t2t.png","tnt.png","transformers-in-vision_files/anchor-4.2.2/anchor.min.js","transformers-in-vision_files/bowser-1.9.3/bowser.min.js","transformers-in-vision_files/distill-2.2.21/template.v2.js","transformers-in-vision_files/header-attrs-2.7/header-attrs.js","transformers-in-vision_files/jquery-1.11.3/jquery.min.js","transformers-in-vision_files/popper-2.6.0/popper.min.js","transformers-in-vision_files/tippy-6.2.7/tippy-bundle.umd.min.js","transformers-in-vision_files/tippy-6.2.7/tippy-light-border.css","transformers-in-vision_files/tippy-6.2.7/tippy.css","transformers-in-vision_files/tippy-6.2.7/tippy.umd.min.js","transformers-in-vision_files/webcomponents-2.0.0/webcomponents.js","vil.png","ViT.gif","ViT.png","vivit.png"]}
  </script>
  <!--radix_placeholder_navigation_in_header-->
<meta name="distill:offset" content="../.."/>

<script type="application/javascript">

  window.headroom_prevent_pin = false;

  window.document.addEventListener("DOMContentLoaded", function (event) {

    // initialize headroom for banner
    var header = $('header').get(0);
    var headerHeight = header.offsetHeight;
    var headroom = new Headroom(header, {
      tolerance: 5,
      onPin : function() {
        if (window.headroom_prevent_pin) {
          window.headroom_prevent_pin = false;
          headroom.unpin();
        }
      }
    });
    headroom.init();
    if(window.location.hash)
      headroom.unpin();
    $(header).addClass('headroom--transition');

    // offset scroll location for banner on hash change
    // (see: https://github.com/WickyNilliams/headroom.js/issues/38)
    window.addEventListener("hashchange", function(event) {
      window.scrollTo(0, window.pageYOffset - (headerHeight + 25));
    });

    // responsive menu
    $('.distill-site-header').each(function(i, val) {
      var topnav = $(this);
      var toggle = topnav.find('.nav-toggle');
      toggle.on('click', function() {
        topnav.toggleClass('responsive');
      });
    });

    // nav dropdowns
    $('.nav-dropbtn').click(function(e) {
      $(this).next('.nav-dropdown-content').toggleClass('nav-dropdown-active');
      $(this).parent().siblings('.nav-dropdown')
         .children('.nav-dropdown-content').removeClass('nav-dropdown-active');
    });
    $("body").click(function(e){
      $('.nav-dropdown-content').removeClass('nav-dropdown-active');
    });
    $(".nav-dropdown").click(function(e){
      e.stopPropagation();
    });
  });
</script>

<style type="text/css">

/* Theme (user-documented overrideables for nav appearance) */

.distill-site-nav {
  color: rgba(255, 255, 255, 0.8);
  background-color: #0F2E3D;
  font-size: 15px;
  font-weight: 300;
}

.distill-site-nav a {
  color: inherit;
  text-decoration: none;
}

.distill-site-nav a:hover {
  color: white;
}

@media print {
  .distill-site-nav {
    display: none;
  }
}

.distill-site-header {

}

.distill-site-footer {

}


/* Site Header */

.distill-site-header {
  width: 100%;
  box-sizing: border-box;
  z-index: 3;
}

.distill-site-header .nav-left {
  display: inline-block;
  margin-left: 8px;
}

@media screen and (max-width: 768px) {
  .distill-site-header .nav-left {
    margin-left: 0;
  }
}


.distill-site-header .nav-right {
  float: right;
  margin-right: 8px;
}

.distill-site-header a,
.distill-site-header .title {
  display: inline-block;
  text-align: center;
  padding: 14px 10px 14px 10px;
}

.distill-site-header .title {
  font-size: 18px;
  min-width: 150px;
}

.distill-site-header .logo {
  padding: 0;
}

.distill-site-header .logo img {
  display: none;
  max-height: 20px;
  width: auto;
  margin-bottom: -4px;
}

.distill-site-header .nav-image img {
  max-height: 18px;
  width: auto;
  display: inline-block;
  margin-bottom: -3px;
}



@media screen and (min-width: 1000px) {
  .distill-site-header .logo img {
    display: inline-block;
  }
  .distill-site-header .nav-left {
    margin-left: 20px;
  }
  .distill-site-header .nav-right {
    margin-right: 20px;
  }
  .distill-site-header .title {
    padding-left: 12px;
  }
}


.distill-site-header .nav-toggle {
  display: none;
}

.nav-dropdown {
  display: inline-block;
  position: relative;
}

.nav-dropdown .nav-dropbtn {
  border: none;
  outline: none;
  color: rgba(255, 255, 255, 0.8);
  padding: 16px 10px;
  background-color: transparent;
  font-family: inherit;
  font-size: inherit;
  font-weight: inherit;
  margin: 0;
  margin-top: 1px;
  z-index: 2;
}

.nav-dropdown-content {
  display: none;
  position: absolute;
  background-color: white;
  min-width: 200px;
  border: 1px solid rgba(0,0,0,0.15);
  border-radius: 4px;
  box-shadow: 0px 8px 16px 0px rgba(0,0,0,0.1);
  z-index: 1;
  margin-top: 2px;
  white-space: nowrap;
  padding-top: 4px;
  padding-bottom: 4px;
}

.nav-dropdown-content hr {
  margin-top: 4px;
  margin-bottom: 4px;
  border: none;
  border-bottom: 1px solid rgba(0, 0, 0, 0.1);
}

.nav-dropdown-active {
  display: block;
}

.nav-dropdown-content a, .nav-dropdown-content .nav-dropdown-header {
  color: black;
  padding: 6px 24px;
  text-decoration: none;
  display: block;
  text-align: left;
}

.nav-dropdown-content .nav-dropdown-header {
  display: block;
  padding: 5px 24px;
  padding-bottom: 0;
  text-transform: uppercase;
  font-size: 14px;
  color: #999999;
  white-space: nowrap;
}

.nav-dropdown:hover .nav-dropbtn {
  color: white;
}

.nav-dropdown-content a:hover {
  background-color: #ddd;
  color: black;
}

.nav-right .nav-dropdown-content {
  margin-left: -45%;
  right: 0;
}

@media screen and (max-width: 768px) {
  .distill-site-header a, .distill-site-header .nav-dropdown  {display: none;}
  .distill-site-header a.nav-toggle {
    float: right;
    display: block;
  }
  .distill-site-header .title {
    margin-left: 0;
  }
  .distill-site-header .nav-right {
    margin-right: 0;
  }
  .distill-site-header {
    overflow: hidden;
  }
  .nav-right .nav-dropdown-content {
    margin-left: 0;
  }
}


@media screen and (max-width: 768px) {
  .distill-site-header.responsive {position: relative; min-height: 500px; }
  .distill-site-header.responsive a.nav-toggle {
    position: absolute;
    right: 0;
    top: 0;
  }
  .distill-site-header.responsive a,
  .distill-site-header.responsive .nav-dropdown {
    display: block;
    text-align: left;
  }
  .distill-site-header.responsive .nav-left,
  .distill-site-header.responsive .nav-right {
    width: 100%;
  }
  .distill-site-header.responsive .nav-dropdown {float: none;}
  .distill-site-header.responsive .nav-dropdown-content {position: relative;}
  .distill-site-header.responsive .nav-dropdown .nav-dropbtn {
    display: block;
    width: 100%;
    text-align: left;
  }
}

/* Site Footer */

.distill-site-footer {
  width: 100%;
  overflow: hidden;
  box-sizing: border-box;
  z-index: 3;
  margin-top: 30px;
  padding-top: 30px;
  padding-bottom: 30px;
  text-align: center;
}

/* Headroom */

d-title {
  padding-top: 6rem;
}

@media print {
  d-title {
    padding-top: 4rem;
  }
}

.headroom {
  z-index: 1000;
  position: fixed;
  top: 0;
  left: 0;
  right: 0;
}

.headroom--transition {
  transition: all .4s ease-in-out;
}

.headroom--unpinned {
  top: -100px;
}

.headroom--pinned {
  top: 0;
}

/* adjust viewport for navbar height */
/* helps vertically center bootstrap (non-distill) content */
.min-vh-100 {
  min-height: calc(100vh - 100px) !important;
}

</style>

<script src="../../site_libs/jquery-1.11.3/jquery.min.js"></script>
<link href="../../site_libs/font-awesome-5.1.0/css/all.css" rel="stylesheet"/>
<link href="../../site_libs/font-awesome-5.1.0/css/v4-shims.css" rel="stylesheet"/>
<script src="../../site_libs/headroom-0.9.4/headroom.min.js"></script>
<!--/radix_placeholder_navigation_in_header-->
  <!--radix_placeholder_distill-->

<style type="text/css">

body {
  background-color: white;
}

.pandoc-table {
  width: 100%;
}

.pandoc-table>caption {
  margin-bottom: 10px;
}

.pandoc-table th:not([align]) {
  text-align: left;
}

.pagedtable-footer {
  font-size: 15px;
}

d-byline .byline {
  grid-template-columns: 2fr 2fr;
}

d-byline .byline h3 {
  margin-block-start: 1.5em;
}

d-byline .byline .authors-affiliations h3 {
  margin-block-start: 0.5em;
}

.authors-affiliations .orcid-id {
  width: 16px;
  height:16px;
  margin-left: 4px;
  margin-right: 4px;
  vertical-align: middle;
  padding-bottom: 2px;
}

d-title .dt-tags {
  margin-top: 1em;
  grid-column: text;
}

.dt-tags .dt-tag {
  text-decoration: none;
  display: inline-block;
  color: rgba(0,0,0,0.6);
  padding: 0em 0.4em;
  margin-right: 0.5em;
  margin-bottom: 0.4em;
  font-size: 70%;
  border: 1px solid rgba(0,0,0,0.2);
  border-radius: 3px;
  text-transform: uppercase;
  font-weight: 500;
}

d-article table.gt_table td,
d-article table.gt_table th {
  border-bottom: none;
}

.html-widget {
  margin-bottom: 2.0em;
}

.l-screen-inset {
  padding-right: 16px;
}

.l-screen .caption {
  margin-left: 10px;
}

.shaded {
  background: rgb(247, 247, 247);
  padding-top: 20px;
  padding-bottom: 20px;
  border-top: 1px solid rgba(0, 0, 0, 0.1);
  border-bottom: 1px solid rgba(0, 0, 0, 0.1);
}

.shaded .html-widget {
  margin-bottom: 0;
  border: 1px solid rgba(0, 0, 0, 0.1);
}

.shaded .shaded-content {
  background: white;
}

.text-output {
  margin-top: 0;
  line-height: 1.5em;
}

.hidden {
  display: none !important;
}

d-article {
  padding-top: 2.5rem;
  padding-bottom: 30px;
}

d-appendix {
  padding-top: 30px;
}

d-article>p>img {
  width: 100%;
}

d-article h2 {
  margin: 1rem 0 1.5rem 0;
}

d-article h3 {
  margin-top: 1.5rem;
}

d-article iframe {
  border: 1px solid rgba(0, 0, 0, 0.1);
  margin-bottom: 2.0em;
  width: 100%;
}

/* Tweak code blocks */

d-article div.sourceCode code,
d-article pre code {
  font-family: Consolas, Monaco, 'Andale Mono', 'Ubuntu Mono', monospace;
}

d-article pre,
d-article div.sourceCode,
d-article div.sourceCode pre {
  overflow: auto;
}

d-article div.sourceCode {
  background-color: white;
}

d-article div.sourceCode pre {
  padding-left: 10px;
  font-size: 12px;
  border-left: 2px solid rgba(0,0,0,0.1);
}

d-article pre {
  font-size: 12px;
  color: black;
  background: none;
  margin-top: 0;
  text-align: left;
  white-space: pre;
  word-spacing: normal;
  word-break: normal;
  word-wrap: normal;
  line-height: 1.5;

  -moz-tab-size: 4;
  -o-tab-size: 4;
  tab-size: 4;

  -webkit-hyphens: none;
  -moz-hyphens: none;
  -ms-hyphens: none;
  hyphens: none;
}

d-article pre a {
  border-bottom: none;
}

d-article pre a:hover {
  border-bottom: none;
  text-decoration: underline;
}

d-article details {
  grid-column: text;
  margin-bottom: 0.8em;
}

@media(min-width: 768px) {

d-article pre,
d-article div.sourceCode,
d-article div.sourceCode pre {
  overflow: visible !important;
}

d-article div.sourceCode pre {
  padding-left: 18px;
  font-size: 14px;
}

d-article pre {
  font-size: 14px;
}

}

figure img.external {
  background: white;
  border: 1px solid rgba(0, 0, 0, 0.1);
  box-shadow: 0 1px 8px rgba(0, 0, 0, 0.1);
  padding: 18px;
  box-sizing: border-box;
}

/* CSS for d-contents */

.d-contents {
  grid-column: text;
  color: rgba(0,0,0,0.8);
  font-size: 0.9em;
  padding-bottom: 1em;
  margin-bottom: 1em;
  padding-bottom: 0.5em;
  margin-bottom: 1em;
  padding-left: 0.25em;
  justify-self: start;
}

@media(min-width: 1000px) {
  .d-contents.d-contents-float {
    height: 0;
    grid-column-start: 1;
    grid-column-end: 4;
    justify-self: center;
    padding-right: 3em;
    padding-left: 2em;
  }
}

.d-contents nav h3 {
  font-size: 18px;
  margin-top: 0;
  margin-bottom: 1em;
}

.d-contents li {
  list-style-type: none
}

.d-contents nav > ul {
  padding-left: 0;
}

.d-contents ul {
  padding-left: 1em
}

.d-contents nav ul li {
  margin-top: 0.6em;
  margin-bottom: 0.2em;
}

.d-contents nav a {
  font-size: 13px;
  border-bottom: none;
  text-decoration: none
  color: rgba(0, 0, 0, 0.8);
}

.d-contents nav a:hover {
  text-decoration: underline solid rgba(0, 0, 0, 0.6)
}

.d-contents nav > ul > li > a {
  font-weight: 600;
}

.d-contents nav > ul > li > ul {
  font-weight: inherit;
}

.d-contents nav > ul > li > ul > li {
  margin-top: 0.2em;
}


.d-contents nav ul {
  margin-top: 0;
  margin-bottom: 0.25em;
}

.d-article-with-toc h2:nth-child(2) {
  margin-top: 0;
}


/* Figure */

.figure {
  position: relative;
  margin-bottom: 2.5em;
  margin-top: 1.5em;
}

.figure img {
  width: 100%;
}

.figure .caption {
  color: rgba(0, 0, 0, 0.6);
  font-size: 12px;
  line-height: 1.5em;
}

.figure img.external {
  background: white;
  border: 1px solid rgba(0, 0, 0, 0.1);
  box-shadow: 0 1px 8px rgba(0, 0, 0, 0.1);
  padding: 18px;
  box-sizing: border-box;
}

.figure .caption a {
  color: rgba(0, 0, 0, 0.6);
}

.figure .caption b,
.figure .caption strong, {
  font-weight: 600;
  color: rgba(0, 0, 0, 1.0);
}

/* Citations */

d-article .citation {
  color: inherit;
  cursor: inherit;
}

div.hanging-indent{
  margin-left: 1em; text-indent: -1em;
}

/* Citation hover box */

.tippy-box[data-theme~=light-border] {
  background-color: rgba(250, 250, 250, 0.95);
}

.tippy-content > p {
  margin-bottom: 0;
  padding: 2px;
}


/* Tweak 1000px media break to show more text */

@media(min-width: 1000px) {
  .base-grid,
  distill-header,
  d-title,
  d-abstract,
  d-article,
  d-appendix,
  distill-appendix,
  d-byline,
  d-footnote-list,
  d-citation-list,
  distill-footer {
    grid-template-columns: [screen-start] 1fr [page-start kicker-start] 80px [middle-start] 50px [text-start kicker-end] 65px 65px 65px 65px 65px 65px 65px 65px [text-end gutter-start] 65px [middle-end] 65px [page-end gutter-end] 1fr [screen-end];
    grid-column-gap: 16px;
  }

  .grid {
    grid-column-gap: 16px;
  }

  d-article {
    font-size: 1.06rem;
    line-height: 1.7em;
  }
  figure .caption, .figure .caption, figure figcaption {
    font-size: 13px;
  }
}

@media(min-width: 1180px) {
  .base-grid,
  distill-header,
  d-title,
  d-abstract,
  d-article,
  d-appendix,
  distill-appendix,
  d-byline,
  d-footnote-list,
  d-citation-list,
  distill-footer {
    grid-template-columns: [screen-start] 1fr [page-start kicker-start] 60px [middle-start] 60px [text-start kicker-end] 60px 60px 60px 60px 60px 60px 60px 60px [text-end gutter-start] 60px [middle-end] 60px [page-end gutter-end] 1fr [screen-end];
    grid-column-gap: 32px;
  }

  .grid {
    grid-column-gap: 32px;
  }
}


/* Get the citation styles for the appendix (not auto-injected on render since
   we do our own rendering of the citation appendix) */

d-appendix .citation-appendix,
.d-appendix .citation-appendix {
  font-size: 11px;
  line-height: 15px;
  border-left: 1px solid rgba(0, 0, 0, 0.1);
  padding-left: 18px;
  border: 1px solid rgba(0,0,0,0.1);
  background: rgba(0, 0, 0, 0.02);
  padding: 10px 18px;
  border-radius: 3px;
  color: rgba(150, 150, 150, 1);
  overflow: hidden;
  margin-top: -12px;
  white-space: pre-wrap;
  word-wrap: break-word;
}

/* Include appendix styles here so they can be overridden */

d-appendix {
  contain: layout style;
  font-size: 0.8em;
  line-height: 1.7em;
  margin-top: 60px;
  margin-bottom: 0;
  border-top: 1px solid rgba(0, 0, 0, 0.1);
  color: rgba(0,0,0,0.5);
  padding-top: 60px;
  padding-bottom: 48px;
}

d-appendix h3 {
  grid-column: page-start / text-start;
  font-size: 15px;
  font-weight: 500;
  margin-top: 1em;
  margin-bottom: 0;
  color: rgba(0,0,0,0.65);
}

d-appendix h3 + * {
  margin-top: 1em;
}

d-appendix ol {
  padding: 0 0 0 15px;
}

@media (min-width: 768px) {
  d-appendix ol {
    padding: 0 0 0 30px;
    margin-left: -30px;
  }
}

d-appendix li {
  margin-bottom: 1em;
}

d-appendix a {
  color: rgba(0, 0, 0, 0.6);
}

d-appendix > * {
  grid-column: text;
}

d-appendix > d-footnote-list,
d-appendix > d-citation-list,
d-appendix > distill-appendix {
  grid-column: screen;
}

/* Include footnote styles here so they can be overridden */

d-footnote-list {
  contain: layout style;
}

d-footnote-list > * {
  grid-column: text;
}

d-footnote-list a.footnote-backlink {
  color: rgba(0,0,0,0.3);
  padding-left: 0.5em;
}



/* Anchor.js */

.anchorjs-link {
  /*transition: all .25s linear; */
  text-decoration: none;
  border-bottom: none;
}
*:hover > .anchorjs-link {
  margin-left: -1.125em !important;
  text-decoration: none;
  border-bottom: none;
}

/* Social footer */

.social_footer {
  margin-top: 30px;
  margin-bottom: 0;
  color: rgba(0,0,0,0.67);
}

.disqus-comments {
  margin-right: 30px;
}

.disqus-comment-count {
  border-bottom: 1px solid rgba(0, 0, 0, 0.4);
  cursor: pointer;
}

#disqus_thread {
  margin-top: 30px;
}

.article-sharing a {
  border-bottom: none;
  margin-right: 8px;
}

.article-sharing a:hover {
  border-bottom: none;
}

.sidebar-section.subscribe {
  font-size: 12px;
  line-height: 1.6em;
}

.subscribe p {
  margin-bottom: 0.5em;
}


.article-footer .subscribe {
  font-size: 15px;
  margin-top: 45px;
}


.sidebar-section.custom {
  font-size: 12px;
  line-height: 1.6em;
}

.custom p {
  margin-bottom: 0.5em;
}

/* Styles for listing layout (hide title) */
.layout-listing d-title, .layout-listing .d-title {
  display: none;
}

/* Styles for posts lists (not auto-injected) */


.posts-with-sidebar {
  padding-left: 45px;
  padding-right: 45px;
}

.posts-list .description h2,
.posts-list .description p {
  font-family: -apple-system, BlinkMacSystemFont, "Segoe UI", Roboto, Oxygen, Ubuntu, Cantarell, "Fira Sans", "Droid Sans", "Helvetica Neue", Arial, sans-serif;
}

.posts-list .description h2 {
  font-weight: 700;
  border-bottom: none;
  padding-bottom: 0;
}

.posts-list h2.post-tag {
  border-bottom: 1px solid rgba(0, 0, 0, 0.2);
  padding-bottom: 12px;
}
.posts-list {
  margin-top: 60px;
  margin-bottom: 24px;
}

.posts-list .post-preview {
  text-decoration: none;
  overflow: hidden;
  display: block;
  border-bottom: 1px solid rgba(0, 0, 0, 0.1);
  padding: 24px 0;
}

.post-preview-last {
  border-bottom: none !important;
}

.posts-list .posts-list-caption {
  grid-column: screen;
  font-weight: 400;
}

.posts-list .post-preview h2 {
  margin: 0 0 6px 0;
  line-height: 1.2em;
  font-style: normal;
  font-size: 24px;
}

.posts-list .post-preview p {
  margin: 0 0 12px 0;
  line-height: 1.4em;
  font-size: 16px;
}

.posts-list .post-preview .thumbnail {
  box-sizing: border-box;
  margin-bottom: 24px;
  position: relative;
  max-width: 500px;
}
.posts-list .post-preview img {
  width: 100%;
  display: block;
}

.posts-list .metadata {
  font-size: 12px;
  line-height: 1.4em;
  margin-bottom: 18px;
}

.posts-list .metadata > * {
  display: inline-block;
}

.posts-list .metadata .publishedDate {
  margin-right: 2em;
}

.posts-list .metadata .dt-authors {
  display: block;
  margin-top: 0.3em;
  margin-right: 2em;
}

.posts-list .dt-tags {
  display: block;
  line-height: 1em;
}

.posts-list .dt-tags .dt-tag {
  display: inline-block;
  color: rgba(0,0,0,0.6);
  padding: 0.3em 0.4em;
  margin-right: 0.2em;
  margin-bottom: 0.4em;
  font-size: 60%;
  border: 1px solid rgba(0,0,0,0.2);
  border-radius: 3px;
  text-transform: uppercase;
  font-weight: 500;
}

.posts-list img {
  opacity: 1;
}

.posts-list img[data-src] {
  opacity: 0;
}

.posts-more {
  clear: both;
}


.posts-sidebar {
  font-size: 16px;
}

.posts-sidebar h3 {
  font-size: 16px;
  margin-top: 0;
  margin-bottom: 0.5em;
  font-weight: 400;
  text-transform: uppercase;
}

.sidebar-section {
  margin-bottom: 30px;
}

.categories ul {
  list-style-type: none;
  margin: 0;
  padding: 0;
}

.categories li {
  color: rgba(0, 0, 0, 0.8);
  margin-bottom: 0;
}

.categories li>a {
  border-bottom: none;
}

.categories li>a:hover {
  border-bottom: 1px solid rgba(0, 0, 0, 0.4);
}

.categories .active {
  font-weight: 600;
}

.categories .category-count {
  color: rgba(0, 0, 0, 0.4);
}


@media(min-width: 768px) {
  .posts-list .post-preview h2 {
    font-size: 26px;
  }
  .posts-list .post-preview .thumbnail {
    float: right;
    width: 30%;
    margin-bottom: 0;
  }
  .posts-list .post-preview .description {
    float: left;
    width: 45%;
  }
  .posts-list .post-preview .metadata {
    float: left;
    width: 20%;
    margin-top: 8px;
  }
  .posts-list .post-preview p {
    margin: 0 0 12px 0;
    line-height: 1.5em;
    font-size: 16px;
  }
  .posts-with-sidebar .posts-list {
    float: left;
    width: 75%;
  }
  .posts-with-sidebar .posts-sidebar {
    float: right;
    width: 20%;
    margin-top: 60px;
    padding-top: 24px;
    padding-bottom: 24px;
  }
}


/* Improve display for browsers without grid (IE/Edge <= 15) */

.downlevel {
  line-height: 1.6em;
  font-family: -apple-system, BlinkMacSystemFont, "Segoe UI", Roboto, Oxygen, Ubuntu, Cantarell, "Fira Sans", "Droid Sans", "Helvetica Neue", Arial, sans-serif;
  margin: 0;
}

.downlevel .d-title {
  padding-top: 6rem;
  padding-bottom: 1.5rem;
}

.downlevel .d-title h1 {
  font-size: 50px;
  font-weight: 700;
  line-height: 1.1em;
  margin: 0 0 0.5rem;
}

.downlevel .d-title p {
  font-weight: 300;
  font-size: 1.2rem;
  line-height: 1.55em;
  margin-top: 0;
}

.downlevel .d-byline {
  padding-top: 0.8em;
  padding-bottom: 0.8em;
  font-size: 0.8rem;
  line-height: 1.8em;
}

.downlevel .section-separator {
  border: none;
  border-top: 1px solid rgba(0, 0, 0, 0.1);
}

.downlevel .d-article {
  font-size: 1.06rem;
  line-height: 1.7em;
  padding-top: 1rem;
  padding-bottom: 2rem;
}


.downlevel .d-appendix {
  padding-left: 0;
  padding-right: 0;
  max-width: none;
  font-size: 0.8em;
  line-height: 1.7em;
  margin-bottom: 0;
  color: rgba(0,0,0,0.5);
  padding-top: 40px;
  padding-bottom: 48px;
}

.downlevel .footnotes ol {
  padding-left: 13px;
}

.downlevel .base-grid,
.downlevel .distill-header,
.downlevel .d-title,
.downlevel .d-abstract,
.downlevel .d-article,
.downlevel .d-appendix,
.downlevel .distill-appendix,
.downlevel .d-byline,
.downlevel .d-footnote-list,
.downlevel .d-citation-list,
.downlevel .distill-footer,
.downlevel .appendix-bottom,
.downlevel .posts-container {
  padding-left: 40px;
  padding-right: 40px;
}

@media(min-width: 768px) {
  .downlevel .base-grid,
  .downlevel .distill-header,
  .downlevel .d-title,
  .downlevel .d-abstract,
  .downlevel .d-article,
  .downlevel .d-appendix,
  .downlevel .distill-appendix,
  .downlevel .d-byline,
  .downlevel .d-footnote-list,
  .downlevel .d-citation-list,
  .downlevel .distill-footer,
  .downlevel .appendix-bottom,
  .downlevel .posts-container {
  padding-left: 150px;
  padding-right: 150px;
  max-width: 900px;
}
}

.downlevel pre code {
  display: block;
  border-left: 2px solid rgba(0, 0, 0, .1);
  padding: 0 0 0 20px;
  font-size: 14px;
}

.downlevel code, .downlevel pre {
  color: black;
  background: none;
  font-family: Consolas, Monaco, 'Andale Mono', 'Ubuntu Mono', monospace;
  text-align: left;
  white-space: pre;
  word-spacing: normal;
  word-break: normal;
  word-wrap: normal;
  line-height: 1.5;

  -moz-tab-size: 4;
  -o-tab-size: 4;
  tab-size: 4;

  -webkit-hyphens: none;
  -moz-hyphens: none;
  -ms-hyphens: none;
  hyphens: none;
}

.downlevel .posts-list .post-preview {
  color: inherit;
}



</style>

<script type="application/javascript">

function is_downlevel_browser() {
  if (bowser.isUnsupportedBrowser({ msie: "12", msedge: "16"},
                                 window.navigator.userAgent)) {
    return true;
  } else {
    return window.load_distill_framework === undefined;
  }
}

// show body when load is complete
function on_load_complete() {

  // add anchors
  if (window.anchors) {
    window.anchors.options.placement = 'left';
    window.anchors.add('d-article > h2, d-article > h3, d-article > h4, d-article > h5');
  }


  // set body to visible
  document.body.style.visibility = 'visible';

  // force redraw for leaflet widgets
  if (window.HTMLWidgets) {
    var maps = window.HTMLWidgets.findAll(".leaflet");
    $.each(maps, function(i, el) {
      var map = this.getMap();
      map.invalidateSize();
      map.eachLayer(function(layer) {
        if (layer instanceof L.TileLayer)
          layer.redraw();
      });
    });
  }

  // trigger 'shown' so htmlwidgets resize
  $('d-article').trigger('shown');
}

function init_distill() {

  init_common();

  // create front matter
  var front_matter = $('<d-front-matter></d-front-matter>');
  $('#distill-front-matter').wrap(front_matter);

  // create d-title
  $('.d-title').changeElementType('d-title');

  // create d-byline
  var byline = $('<d-byline></d-byline>');
  $('.d-byline').replaceWith(byline);

  // create d-article
  var article = $('<d-article></d-article>');
  $('.d-article').wrap(article).children().unwrap();

  // move posts container into article
  $('.posts-container').appendTo($('d-article'));

  // create d-appendix
  $('.d-appendix').changeElementType('d-appendix');

  // flag indicating that we have appendix items
  var appendix = $('.appendix-bottom').children('h3').length > 0;

  // replace footnotes with <d-footnote>
  $('.footnote-ref').each(function(i, val) {
    appendix = true;
    var href = $(this).attr('href');
    var id = href.replace('#', '');
    var fn = $('#' + id);
    var fn_p = $('#' + id + '>p');
    fn_p.find('.footnote-back').remove();
    var text = fn_p.html();
    var dtfn = $('<d-footnote></d-footnote>');
    dtfn.html(text);
    $(this).replaceWith(dtfn);
  });
  // remove footnotes
  $('.footnotes').remove();

  // move refs into #references-listing
  $('#references-listing').replaceWith($('#refs'));

  $('h1.appendix, h2.appendix').each(function(i, val) {
    $(this).changeElementType('h3');
  });
  $('h3.appendix').each(function(i, val) {
    var id = $(this).attr('id');
    $('.d-contents a[href="#' + id + '"]').parent().remove();
    appendix = true;
    $(this).nextUntil($('h1, h2, h3')).addBack().appendTo($('d-appendix'));
  });

  // show d-appendix if we have appendix content
  $("d-appendix").css('display', appendix ? 'grid' : 'none');

  // localize layout chunks to just output
  $('.layout-chunk').each(function(i, val) {

    // capture layout
    var layout = $(this).attr('data-layout');

    // apply layout to markdown level block elements
    var elements = $(this).children().not('details, div.sourceCode, pre, script');
    elements.each(function(i, el) {
      var layout_div = $('<div class="' + layout + '"></div>');
      if (layout_div.hasClass('shaded')) {
        var shaded_content = $('<div class="shaded-content"></div>');
        $(this).wrap(shaded_content);
        $(this).parent().wrap(layout_div);
      } else {
        $(this).wrap(layout_div);
      }
    });


    // unwrap the layout-chunk div
    $(this).children().unwrap();
  });

  // remove code block used to force  highlighting css
  $('.distill-force-highlighting-css').parent().remove();

  // remove empty line numbers inserted by pandoc when using a
  // custom syntax highlighting theme
  $('code.sourceCode a:empty').remove();

  // load distill framework
  load_distill_framework();

  // wait for window.distillRunlevel == 4 to do post processing
  function distill_post_process() {

    if (!window.distillRunlevel || window.distillRunlevel < 4)
      return;

    // hide author/affiliations entirely if we have no authors
    var front_matter = JSON.parse($("#distill-front-matter").html());
    var have_authors = front_matter.authors && front_matter.authors.length > 0;
    if (!have_authors)
      $('d-byline').addClass('hidden');

    // article with toc class
    $('.d-contents').parent().addClass('d-article-with-toc');

    // strip links that point to #
    $('.authors-affiliations').find('a[href="#"]').removeAttr('href');

    // add orcid ids
    $('.authors-affiliations').find('.author').each(function(i, el) {
      var orcid_id = front_matter.authors[i].orcidID;
      if (orcid_id) {
        var a = $('<a></a>');
        a.attr('href', 'https://orcid.org/' + orcid_id);
        var img = $('<img></img>');
        img.addClass('orcid-id');
        img.attr('alt', 'ORCID ID');
        img.attr('src','data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAABAAAAAQCAYAAAAf8/9hAAAAGXRFWHRTb2Z0d2FyZQBBZG9iZSBJbWFnZVJlYWR5ccllPAAAA2ZpVFh0WE1MOmNvbS5hZG9iZS54bXAAAAAAADw/eHBhY2tldCBiZWdpbj0i77u/IiBpZD0iVzVNME1wQ2VoaUh6cmVTek5UY3prYzlkIj8+IDx4OnhtcG1ldGEgeG1sbnM6eD0iYWRvYmU6bnM6bWV0YS8iIHg6eG1wdGs9IkFkb2JlIFhNUCBDb3JlIDUuMC1jMDYwIDYxLjEzNDc3NywgMjAxMC8wMi8xMi0xNzozMjowMCAgICAgICAgIj4gPHJkZjpSREYgeG1sbnM6cmRmPSJodHRwOi8vd3d3LnczLm9yZy8xOTk5LzAyLzIyLXJkZi1zeW50YXgtbnMjIj4gPHJkZjpEZXNjcmlwdGlvbiByZGY6YWJvdXQ9IiIgeG1sbnM6eG1wTU09Imh0dHA6Ly9ucy5hZG9iZS5jb20veGFwLzEuMC9tbS8iIHhtbG5zOnN0UmVmPSJodHRwOi8vbnMuYWRvYmUuY29tL3hhcC8xLjAvc1R5cGUvUmVzb3VyY2VSZWYjIiB4bWxuczp4bXA9Imh0dHA6Ly9ucy5hZG9iZS5jb20veGFwLzEuMC8iIHhtcE1NOk9yaWdpbmFsRG9jdW1lbnRJRD0ieG1wLmRpZDo1N0NEMjA4MDI1MjA2ODExOTk0QzkzNTEzRjZEQTg1NyIgeG1wTU06RG9jdW1lbnRJRD0ieG1wLmRpZDozM0NDOEJGNEZGNTcxMUUxODdBOEVCODg2RjdCQ0QwOSIgeG1wTU06SW5zdGFuY2VJRD0ieG1wLmlpZDozM0NDOEJGM0ZGNTcxMUUxODdBOEVCODg2RjdCQ0QwOSIgeG1wOkNyZWF0b3JUb29sPSJBZG9iZSBQaG90b3Nob3AgQ1M1IE1hY2ludG9zaCI+IDx4bXBNTTpEZXJpdmVkRnJvbSBzdFJlZjppbnN0YW5jZUlEPSJ4bXAuaWlkOkZDN0YxMTc0MDcyMDY4MTE5NUZFRDc5MUM2MUUwNEREIiBzdFJlZjpkb2N1bWVudElEPSJ4bXAuZGlkOjU3Q0QyMDgwMjUyMDY4MTE5OTRDOTM1MTNGNkRBODU3Ii8+IDwvcmRmOkRlc2NyaXB0aW9uPiA8L3JkZjpSREY+IDwveDp4bXBtZXRhPiA8P3hwYWNrZXQgZW5kPSJyIj8+84NovQAAAR1JREFUeNpiZEADy85ZJgCpeCB2QJM6AMQLo4yOL0AWZETSqACk1gOxAQN+cAGIA4EGPQBxmJA0nwdpjjQ8xqArmczw5tMHXAaALDgP1QMxAGqzAAPxQACqh4ER6uf5MBlkm0X4EGayMfMw/Pr7Bd2gRBZogMFBrv01hisv5jLsv9nLAPIOMnjy8RDDyYctyAbFM2EJbRQw+aAWw/LzVgx7b+cwCHKqMhjJFCBLOzAR6+lXX84xnHjYyqAo5IUizkRCwIENQQckGSDGY4TVgAPEaraQr2a4/24bSuoExcJCfAEJihXkWDj3ZAKy9EJGaEo8T0QSxkjSwORsCAuDQCD+QILmD1A9kECEZgxDaEZhICIzGcIyEyOl2RkgwAAhkmC+eAm0TAAAAABJRU5ErkJggg==');
        a.append(img);
        $(this).append(a);
      }
    });

    // hide elements of author/affiliations grid that have no value
    function hide_byline_column(caption) {
      $('d-byline').find('h3:contains("' + caption + '")').parent().css('visibility', 'hidden');
    }

    // affiliations
    var have_affiliations = false;
    for (var i = 0; i<front_matter.authors.length; ++i) {
      var author = front_matter.authors[i];
      if (author.affiliation !== "&nbsp;") {
        have_affiliations = true;
        break;
      }
    }
    if (!have_affiliations)
      $('d-byline').find('h3:contains("Affiliations")').css('visibility', 'hidden');

    // published date
    if (!front_matter.publishedDate)
      hide_byline_column("Published");

    // document object identifier
    var doi = $('d-byline').find('h3:contains("DOI")');
    var doi_p = doi.next().empty();
    if (!front_matter.doi) {
      // if we have a citation and valid citationText then link to that
      if ($('#citation').length > 0 && front_matter.citationText) {
        doi.html('Citation');
        $('<a href="#citation"></a>')
          .text(front_matter.citationText)
          .appendTo(doi_p);
      } else {
        hide_byline_column("DOI");
      }
    } else {
      $('<a></a>')
         .attr('href', "https://doi.org/" + front_matter.doi)
         .html(front_matter.doi)
         .appendTo(doi_p);
    }

     // change plural form of authors/affiliations
    if (front_matter.authors.length === 1) {
      var grid = $('.authors-affiliations');
      grid.children('h3:contains("Authors")').text('Author');
      grid.children('h3:contains("Affiliations")').text('Affiliation');
    }

    // remove d-appendix and d-footnote-list local styles
    $('d-appendix > style:first-child').remove();
    $('d-footnote-list > style:first-child').remove();

    // move appendix-bottom entries to the bottom
    $('.appendix-bottom').appendTo('d-appendix').children().unwrap();
    $('.appendix-bottom').remove();

    // hoverable references
    $('span.citation[data-cites]').each(function() {
      var refHtml = $('#ref-' + $(this).attr('data-cites')).html();
      window.tippy(this, {
        allowHTML: true,
        content: refHtml,
        maxWidth: 500,
        interactive: true,
        interactiveBorder: 10,
        theme: 'light-border',
        placement: 'bottom-start'
      });
    });

    // clear polling timer
    clearInterval(tid);

    // show body now that everything is ready
    on_load_complete();
  }

  var tid = setInterval(distill_post_process, 50);
  distill_post_process();

}

function init_downlevel() {

  init_common();

   // insert hr after d-title
  $('.d-title').after($('<hr class="section-separator"/>'));

  // check if we have authors
  var front_matter = JSON.parse($("#distill-front-matter").html());
  var have_authors = front_matter.authors && front_matter.authors.length > 0;

  // manage byline/border
  if (!have_authors)
    $('.d-byline').remove();
  $('.d-byline').after($('<hr class="section-separator"/>'));
  $('.d-byline a').remove();

  // remove toc
  $('.d-contents').remove();

  // move appendix elements
  $('h1.appendix, h2.appendix').each(function(i, val) {
    $(this).changeElementType('h3');
  });
  $('h3.appendix').each(function(i, val) {
    $(this).nextUntil($('h1, h2, h3')).addBack().appendTo($('.d-appendix'));
  });


  // inject headers into references and footnotes
  var refs_header = $('<h3></h3>');
  refs_header.text('References');
  $('#refs').prepend(refs_header);

  var footnotes_header = $('<h3></h3');
  footnotes_header.text('Footnotes');
  $('.footnotes').children('hr').first().replaceWith(footnotes_header);

  // move appendix-bottom entries to the bottom
  $('.appendix-bottom').appendTo('.d-appendix').children().unwrap();
  $('.appendix-bottom').remove();

  // remove appendix if it's empty
  if ($('.d-appendix').children().length === 0)
    $('.d-appendix').remove();

  // prepend separator above appendix
  $('.d-appendix').before($('<hr class="section-separator" style="clear: both"/>'));

  // trim code
  $('pre>code').each(function(i, val) {
    $(this).html($.trim($(this).html()));
  });

  // move posts-container right before article
  $('.posts-container').insertBefore($('.d-article'));

  $('body').addClass('downlevel');

  on_load_complete();
}


function init_common() {

  // jquery plugin to change element types
  (function($) {
    $.fn.changeElementType = function(newType) {
      var attrs = {};

      $.each(this[0].attributes, function(idx, attr) {
        attrs[attr.nodeName] = attr.nodeValue;
      });

      this.replaceWith(function() {
        return $("<" + newType + "/>", attrs).append($(this).contents());
      });
    };
  })(jQuery);

  // prevent underline for linked images
  $('a > img').parent().css({'border-bottom' : 'none'});

  // mark non-body figures created by knitr chunks as 100% width
  $('.layout-chunk').each(function(i, val) {
    var figures = $(this).find('img, .html-widget');
    if ($(this).attr('data-layout') !== "l-body") {
      figures.css('width', '100%');
    } else {
      figures.css('max-width', '100%');
      figures.filter("[width]").each(function(i, val) {
        var fig = $(this);
        fig.css('width', fig.attr('width') + 'px');
      });

    }
  });

  // auto-append index.html to post-preview links in file: protocol
  // and in rstudio ide preview
  $('.post-preview').each(function(i, val) {
    if (window.location.protocol === "file:")
      $(this).attr('href', $(this).attr('href') + "index.html");
  });

  // get rid of index.html references in header
  if (window.location.protocol !== "file:") {
    $('.distill-site-header a[href]').each(function(i,val) {
      $(this).attr('href', $(this).attr('href').replace("index.html", "./"));
    });
  }

  // add class to pandoc style tables
  $('tr.header').parent('thead').parent('table').addClass('pandoc-table');
  $('.kable-table').children('table').addClass('pandoc-table');

  // add figcaption style to table captions
  $('caption').parent('table').addClass("figcaption");

  // initialize posts list
  if (window.init_posts_list)
    window.init_posts_list();

  // implmement disqus comment link
  $('.disqus-comment-count').click(function() {
    window.headroom_prevent_pin = true;
    $('#disqus_thread').toggleClass('hidden');
    if (!$('#disqus_thread').hasClass('hidden')) {
      var offset = $(this).offset();
      $(window).resize();
      $('html, body').animate({
        scrollTop: offset.top - 35
      });
    }
  });
}

document.addEventListener('DOMContentLoaded', function() {
  if (is_downlevel_browser())
    init_downlevel();
  else
    window.addEventListener('WebComponentsReady', init_distill);
});

</script>

<!--/radix_placeholder_distill-->
  <script src="../../site_libs/header-attrs-2.7/header-attrs.js"></script>
  <script src="../../site_libs/jquery-1.11.3/jquery.min.js"></script>
  <script src="../../site_libs/popper-2.6.0/popper.min.js"></script>
  <link href="../../site_libs/tippy-6.2.7/tippy.css" rel="stylesheet" />
  <link href="../../site_libs/tippy-6.2.7/tippy-light-border.css" rel="stylesheet" />
  <script src="../../site_libs/tippy-6.2.7/tippy.umd.min.js"></script>
  <script src="../../site_libs/anchor-4.2.2/anchor.min.js"></script>
  <script src="../../site_libs/bowser-1.9.3/bowser.min.js"></script>
  <script src="../../site_libs/webcomponents-2.0.0/webcomponents.js"></script>
  <script src="../../site_libs/distill-2.2.21/template.v2.js"></script>
  <!--radix_placeholder_site_in_header-->
<!--/radix_placeholder_site_in_header-->


</head>

<body>

<!--radix_placeholder_front_matter-->

<script id="distill-front-matter" type="text/json">
{"title":"Transformers in Vision","description":"What have Vision Transformers been up to?","authors":[{"author":"Niccolò Zanichelli","authorURL":"https://twitter.com/nickz_42","affiliation":"University of Parma","affiliationURL":"#","orcidID":""}],"publishedDate":"2021-04-28T00:00:00.000+02:00","citationText":"Zanichelli, 2021"}
</script>

<!--/radix_placeholder_front_matter-->
<!--radix_placeholder_navigation_before_body-->
<header class="header header--fixed" role="banner">
<nav class="distill-site-nav distill-site-header">
<div class="nav-left">
<a href="../../index.html" class="title">IAML Distill Blog</a>
</div>
<div class="nav-right">
<a href="../../index.html">Home</a>
<a href="../../about.html">About</a>
<a href="javascript:void(0);" class="nav-toggle">&#9776;</a>
</div>
</nav>
</header>
<!--/radix_placeholder_navigation_before_body-->
<!--radix_placeholder_site_before_body-->
<!--/radix_placeholder_site_before_body-->

<div class="d-title">
<h1>Transformers in Vision</h1>
<!--radix_placeholder_categories-->
<!--/radix_placeholder_categories-->
<p><p>What have Vision Transformers been up to?</p></p>
</div>

<div class="d-byline">
  
  true
  
<br/>04-28-2021
</div>

<div class="d-article">
<div class="d-contents d-contents-float">
<nav class="l-text toc figcaption" id="TOC">
<h3>Contents</h3>
<ul>
<li><a href="#introduction">Introduction</a></li>
<li><a href="#overview">Overview</a></li>
<li><a href="#vision-transformers-for-image-recognition">Vision Transformers for Image Recognition</a></li>
<li><a href="#a-comparison-for-image-classification">A Comparison for Image Classification</a></li>
<li><a href="#robustness-and-equivariance">Robustness and Equivariance</a></li>
<li><a href="#vision-transformers-in-self-supervised-learning">Vision Transformers in Self-Supervised Learning</a></li>
<li><a href="#vision-transformers-in-medical-research">Vision Transformers in Medical Research</a></li>
<li><a href="#vision-transformers-for-video-recognition">Vision Transformers for Video Recognition</a></li>
<li><a href="#conclusion">Conclusion</a></li>
<li><a href="#acknowledgements">Acknowledgements</a></li>
</ul>
</nav>
</div>
<h2 id="introduction">Introduction</h2>
<p>Initially introduced with the now-famous <a href="(https://arxiv.org/abs/1706.03762)"><strong>Attention is all you need</strong></a><span class="citation" data-cites="transformer"><sup><a href="#ref-transformer" role="doc-biblioref">1</a></sup></span>, the Transformer has dominated the field of Natural Language Processing (NLP) for years. Particularly worth noting is the effort gone into scaling up Transformer-based models, such as <a href="https://doi.org/10.18653/v1/n19-1423">BERT</a><span class="citation" data-cites="bert"><sup><a href="#ref-bert" role="doc-biblioref">2</a></sup></span>, <a href="http://arxiv.org/abs/1909.08053">MegatronLM</a><span class="citation" data-cites="megatron"><sup><a href="#ref-megatron" role="doc-biblioref">3</a></sup></span>, <a href="http://jmlr.org/papers/v21/20-074.html">T5</a><span class="citation" data-cites="t5"><sup><a href="#ref-t5" role="doc-biblioref">4</a></sup></span>, and the various GPTs (<a href="https://openai.com/blog/language-unsupervised/">GPT</a><span class="citation" data-cites="gpt"><sup><a href="#ref-gpt" role="doc-biblioref">5</a></sup></span>, <a href="https://openai.com/blog/better-language-models/">GPT-2</a><span class="citation" data-cites="gpt2"><sup><a href="#ref-gpt2" role="doc-biblioref">6</a></sup></span> and <a href="https://proceedings.neurips.cc/paper/2020/hash/1457c0d6bfcb4967418bfb8ac142f64a-Abstract.html">GPT-3</a><span class="citation" data-cites="gpt3"><sup><a href="#ref-gpt3" role="doc-biblioref">7</a></sup></span>), due to their favourable scaling characteristics<span class="citation" data-cites="neural_laws1 neural_laws2"><sup><a href="#ref-neural_laws1" role="doc-biblioref">8</a>,<a href="#ref-neural_laws2" role="doc-biblioref">9</a></sup></span>.</p>
<p>The success of transformers in NLP has not gone unnoticed in other fields, where they have been responsible for significant breakthroughs such as <a href="https://deepmind.com/blog/article/alphafold-a-solution-to-a-50-year-old-grand-challenge-in-biology">AlphaFold 2</a> in the field of protein folding.</p>
<p>Important works adapting transformers (and self-attention) to vision include <a href="https://doi.org/10.1109/ICCV.2019.00338">Attention Augmented Convolutional Networks</a><span class="citation" data-cites="aacn"><sup><a href="#ref-aacn" role="doc-biblioref">10</a></sup></span>, <a href="https://proceedings.neurips.cc/paper/2019/hash/3416a75f4cea9109507cacd8e2f2aefc-Abstract.html">Stand-Alone Self-Attention models</a><span class="citation" data-cites="sasa"><sup><a href="#ref-sasa" role="doc-biblioref">11</a></sup></span> (SASA models), <a href="https://link.springer.com/chapter/10.1007/978-3-030-58452-8_13">DETR</a><span class="citation" data-cites="detr"><sup><a href="#ref-detr" role="doc-biblioref">12</a></sup></span>, <a href="https://arxiv.org/abs/2006.03677">Visual Transformers</a><span class="citation" data-cites="visual_transformer"><sup><a href="#ref-visual_transformer" role="doc-biblioref">13</a></sup></span> and <a href="https://arxiv.org/abs/2102.08602">LambdaNetworks</a><span class="citation" data-cites="lambda"><sup><a href="#ref-lambda" role="doc-biblioref">14</a></sup></span>; as well as <a href="http://proceedings.mlr.press/v80/parmar18a.html">Image Transformers</a><span class="citation" data-cites="image_transformer"><sup><a href="#ref-image_transformer" role="doc-biblioref">15</a></sup></span> and <a href="http://arxiv.org/abs/1912.12180">Axial Transformers</a><span class="citation" data-cites="axial"><sup><a href="#ref-axial" role="doc-biblioref">16</a></sup></span> in the generative domain.</p>
<p>For an in-depth introduction to these works, we recommend two recent reviews: <a href="https://arxiv.org/abs/2012.12556"><strong>A Survey in Visual Transformers</strong></a><span class="citation" data-cites="survey1"><sup><a href="#ref-survey1" role="doc-biblioref">17</a></sup></span> and <a href="https://arxiv.org/abs/2101.01169"><strong>Transformers in Vision: A Survey</strong></a><span class="citation" data-cites="survey2"><sup><a href="#ref-survey2" role="doc-biblioref">18</a></sup></span>.</p>
<h2 id="overview">Overview</h2>
<p>This blog post aims to summarize recent research in applying transformers and self-attention to vision, with a focus on (but scope not limited to) image classification. While by no means exhaustive, it can hopefully represent a starting point for a more in-depth dive into the literature.</p>
<p>We first introduce the Vision Transformer, a simple yet powerful architecture that has had a significant influence on recent research due to its performance in large data regimes. We then continue with the many works studying how to achieve similar high performance using transformers (and self-attention) when data is not as plentiful. Finally, we discuss papers studying the robustness of these models to perturbations as well as their performance in self-supervised, medical and video tasks.</p>
<p>Figures are taken from their respective papers unless the source is explicitly provided in the caption.</p>
<p>Before we begin, it is worth noting that familiarity with transformers and self-attention is recommended; great resources include Jay Alammar’s <a href="http://jalammar.github.io/illustrated-transformer/">Illustrated Transformer</a> and Peter Bloem’s <a href="http://peterbloem.nl/blog/transformers">Transformers from scratch</a>. For high-quality implementations of many models discussed in this post, check out Ross Wightman’s <a href="https://github.com/rwightman/pytorch-image-models">PyTorch Image Models</a> as well as <a href="https://github.com/lucidrains/">Phil Wang’s work</a>.</p>
<h2 id="vision-transformers-for-image-recognition">Vision Transformers for Image Recognition</h2>
<h3 id="an-image-is-worth-16x16-words-the-vision-transformer">An image is worth 16x16 words: the Vision Transformer</h3>
<p>First introduced in <a href="https://openreview.net/forum?id=YicbFdNTTy"><strong>An Image is Worth 16x16 Words: Transformers for Image Recognition at Scale</strong></a><span class="citation" data-cites="vit"><sup><a href="#ref-vit" role="doc-biblioref">19</a></sup></span>, Vision Transformers (ViTs) have taken computer vision by storm, leading to hundreds of citations in the span of a few months. The paper’s main goal was to show that a vanilla Transformer, once adapted to deal with data from the visual domain, could compete with some of the most performant convolutional neural networks (CNNs) developed up to that point.</p>
<p>The Vision Transformer architecture is conceptually simple: divide the image into patches, flatten and project them into a <span class="math inline">\(D\)</span>-dimensional embedding space obtaining the so-called patch embeddings, add positional embeddings (a set of learnable vectors allowing the model to retain positional information) and concatenate a (learnable) class token, then let the Transformer encoder do its magic. Finally, a classification head is applied to the class token to obtain the model’s logits.</p>
<center>
<div class="layout-chunk" data-layout="l-body">
<div class="figure"><span id="fig:unnamed-chunk-1"></span>
<img src="ViT.gif" alt="A Vision Transformer classifying an image. [Source.](https://ai.googleblog.com/2020/12/transformers-for-image-recognition-at.html)"  />
<p class="caption">
Figure 1: A Vision Transformer classifying an image. <a href="https://ai.googleblog.com/2020/12/transformers-for-image-recognition-at.html">Source.</a>
</p>
</div>
</div>
</center>
<p>The model’s performance was acceptable when trained on ImageNet (<span class="math inline">\(1\)</span>M images), great when pre-trained on ImageNet-21k (<span class="math inline">\(14\)</span>M images), and state-of-the-art when pre-trained on Google’s internal JFT-300M dataset (<span class="math inline">\(300\)</span>M images).</p>
<p>The striking performance improvement was due to the reduced inductive bias that characterizes Vision Transformers. By making fewer assumptions about the data, Vision Transformers could better adapt themselves to the given task. However, this ability came at a cost – when the sample size was too small (such as in the ImageNet case), the models overfit, resulting in degraded performance.</p>
<p>The goal for many follow-up papers would be that of matching (and surpassing) the performance of the best convolutional models in the “small” data regime – ImageNet (which is after all over a million images) and below.</p>
<h3 id="stronger-data-augmentation-allows-more-efficient-learning">Stronger data augmentation allows more efficient learning</h3>
<p><a href="https://arxiv.org/abs/2012.12877"><strong>Training data-efficient image transformers &amp; distillation through attention</strong></a><span class="citation" data-cites="deit"><sup><a href="#ref-deit" role="doc-biblioref">20</a></sup></span> was the first paper to show that models based on ViTs could be competitive on ImageNet without access to additional data.</p>
<p>Two main contributions characterize the paper:</p>
<ul>
<li>A novel training recipe (referred to below as the DeiT recipe), characterized by more substantial data augmentation and stochastic depth<span class="citation" data-cites="depth"><sup><a href="#ref-depth" role="doc-biblioref">21</a></sup></span>. These changes introduce additional regularization limiting ViT’s tendency to overfit in the small data regime, thus boosting its performance. The authors recommend the use of Rand-Augment<span class="citation" data-cites="randaugment"><sup><a href="#ref-randaugment" role="doc-biblioref">22</a></sup></span>, Mixup<span class="citation" data-cites="mixup"><sup><a href="#ref-mixup" role="doc-biblioref">23</a></sup></span>, CutMix<span class="citation" data-cites="cutmix"><sup><a href="#ref-cutmix" role="doc-biblioref">24</a></sup></span>, and Random Erasing<span class="citation" data-cites="randerasing"><sup><a href="#ref-randerasing" role="doc-biblioref">25</a></sup></span>; they also recommend not to use DropOut<span class="citation" data-cites="dropout"><sup><a href="#ref-dropout" role="doc-biblioref">26</a></sup></span>.</li>
<li>Hard-label distillation. In this approach, an additional learnable token, called the distillation token, is concatenated to the patch embeddings. The model is then trained with the loss function <span class="math display">\[\mathcal{L}_{hardDistill}=\frac{1}{2}\mathcal{L}_{CE}(\sigma(Z_{cls}), y_{true}) + \frac{1}{2}\mathcal{L}_{CE}(\sigma(Z_{distill}), y_{teacher})\]</span> where <span class="math inline">\(\mathcal{L}_{CE}\)</span> is the cross-entropy loss function, <span class="math inline">\(\sigma\)</span> is the softmax function, <span class="math inline">\(Z_{cls}\)</span> and <span class="math inline">\(Z_{distill}\)</span> are the student model’s logits derived respectively from the class and distillation tokens, and <span class="math inline">\(y_{true}\)</span> and <span class="math inline">\(y_{teacher}\)</span> are respectively the true and the teacher’s hard labels. This distillation technique allows the model to learn even when the combination of multiple strong data augmentations causes the provided label to be imprecise, as the teacher network will produce the most probable label. Interestingly, the authors found CNNs to be better teacher networks than other Vision Transformers.</li>
</ul>
<div class="layout-chunk" data-layout="l-body">
<div class="figure" style="text-align: center"><span id="fig:unnamed-chunk-2"></span>
<img src="deit.png" alt="The Data efficient image Transformer hard-label distillation procedure." width="100%" />
<p class="caption">
Figure 2: The Data efficient image Transformer hard-label distillation procedure.
</p>
</div>
</div>
<p>The resulting models, called Data efficient image Transformers (DeiTs), were competitive with EfficientNet on the accuracy/step time trade-off, proving that ViT-based models could compete with highly performant CNNs even in the ImageNet data regime. It is however worth noting that DeiTs performed much worse than <a href="http://proceedings.mlr.press/v97/tan19a.html">EfficientNets</a><span class="citation" data-cites="efficientnetv1"><sup><a href="#ref-efficientnetv1" role="doc-biblioref">27</a></sup></span> in terms of accuracy/parameters and accuracy/FLOPs.</p>
<h3 id="incorporating-relative-self-attention-in-resnet-design">Incorporating relative Self-Attention in ResNet design</h3>
<p><a href="https://arxiv.org/abs/2101.11605"><strong>Bottleneck Transformers for Visual Recognition</strong></a><span class="citation" data-cites="botnet"><sup><a href="#ref-botnet" role="doc-biblioref">28</a></sup></span> investigated a family of hybrid convolution and attention models obtained by incorporating Multi-Head Self-Attention (MHSA) in ResNet designs. In particular, the authors showed that by simply replacing the <span class="math inline">\(3\times 3\)</span> convolutions with relative position MHSA layers in the bottleneck blocks of the last stage of a ResNet, it was possible to obtain an improvement over several baselines.</p>
<p>Once additional bells-and-whistles were added, such as <a href="https://openreview.net/forum?id=ryE2Slzu-B">Squeeze-and-Excite layers</a><span class="citation" data-cites="se"><sup><a href="#ref-se" role="doc-biblioref">29</a></sup></span> and <a href="https://arxiv.org/abs/1702.03118v3">SiLU non-linearities</a><span class="citation" data-cites="silu"><sup><a href="#ref-silu" role="doc-biblioref">30</a></sup></span>, the models (that the authors call Bottleneck Transformer Networks or BoTNets) demonstrated favorable scaling, outperforming EfficientNets in the accuracy/step-time trade-off beyond 83% top-1 accuracy. The worse performance of small BoTNet models could be due to the fact that the authors do not employ the DeiT training recipe to combat overfitting.</p>
<p>Further, the use of ResNet blocks in the initial stages to efficiently learn lower resolution feature maps allowed the models to perform well in both instance segmentation and object detection tasks, where high-resolution images are typical.</p>
<h3 id="conditional-positional-encodings">Conditional Positional Encodings</h3>
<p><a href="https://arxiv.org/abs/2102.10882"><strong>Conditional Positional Encodings for Vision Transformers</strong></a><span class="citation" data-cites="conditional"><sup><a href="#ref-conditional" role="doc-biblioref">31</a></sup></span> studied alternatives to the positional embeddings and class token used in ViTs. In particular, the paper proposes the use of Positional Encodings Generators (PEGs), a module that produces positional encodings dynamically, and the use of global average pooling as an alternative to the (non-translation-invariant) class token.</p>
<p>PEGs reshape the flattened input sequence to 2D, apply a series of convolutional layers (the authors use depthwise separable convolutions) with zero-padding, and flatten the result. Positional information is thus introduced by the presence of zero-padding, unlike in ViTs, where 1D learnable positional embeddings are used. The method has two main advantages:</p>
<ul>
<li>The model is now translation invariant.</li>
<li>The model can now be used as-is on higher resolutions, unlike normal ViTs that require rescaling of the positional embeddings before fine-tuning.</li>
</ul>
<center>
<div class="layout-chunk" data-layout="l-body">
<div class="figure"><span id="fig:unnamed-chunk-3"></span>
<img src="conditional.png" alt="The Positional Encoding Generator (PEG) module architecture." width="100%" />
<p class="caption">
Figure 3: The Positional Encoding Generator (PEG) module architecture.
</p>
</div>
</div>
</center>
<p>The resulting models, called Conditional Positional Encoding Vision Transformers (CPVTs), are trained with the DeiT recipe and obtain a small boost in performance (particularly when tested on higher resolution images without fine-tuning).</p>
<h3 id="modeling-the-local-patch-structure">Modeling the local patch structure</h3>
<p><a href="https://arxiv.org/abs/2103.00112"><strong>Transformer in Transformer</strong></a><span class="citation" data-cites="tnt"><sup><a href="#ref-tnt" role="doc-biblioref">32</a></sup></span> (TNT) studied the importance of intra-patch structure by introducing an additional transformer block, dedicated to pixel embeddings, inside the transformer blocks used in ViT.</p>
<div class="layout-chunk" data-layout="l-body">
<div class="figure" style="text-align: center"><span id="fig:unnamed-chunk-4"></span>
<img src="tnt.png" alt="The Transformer in Transformer (TNT) architecture." width="100%" />
<p class="caption">
Figure 4: The Transformer in Transformer (TNT) architecture.
</p>
</div>
</div>
<p>The output of the inner transformer block is both sent as-is to the next layer and adapted for use in the outer transformer block which can now take into account both inter-patch relationships (like ViT does) and intra-patch structure. The authors also introduced a separate set of positional embeddings, added to the pixel embeddings before entering the encoder.</p>
<p>The models, trained with the DeiT recipe, outperformed both ViTs and DeiTs on ImageNet, achieving higher parameter and FLOP efficiency, although not quite as high as EfficientNet’s.</p>
<h3 id="deeper-and-deeper-with-vision-transformers">Deeper and deeper with Vision Transformers</h3>
<p><a href="https://arxiv.org/abs/2103.11886"><strong>DeepViT: Towards Deeper Vision Transformer</strong></a><span class="citation" data-cites="deepvit"><sup><a href="#ref-deepvit" role="doc-biblioref">33</a></sup></span> investigated how the performance of ViTs changed with increasing depth. After 24 transformer blocks, the authors discovered that adding additional blocks did not lead to improved performance, as both the feature maps and the attention matrices end up being remarkably similar to each other, a problem they name Attention Collapse.</p>
<p>To solve this issue, the authors proposed a simple variant of self-attention that they call Re-Attention, characterized by a learnable transformation matrix <span class="math inline">\(\Theta \in R^{H \times H}\)</span> (where <span class="math inline">\(H\)</span> is the number of attention heads) applied directly after the softmax. This matrix (which is shared throughout the network but can also be layer-specific) allows the model to establish cross-head communication, diminishing inter-layer feature map similarity.</p>
<p>The authors also reported positive results following an approach similar to the one proposed in <a href="https://arxiv.org/abs/2102.12702">LazyFormer</a><span class="citation" data-cites="lazy"><sup><a href="#ref-lazy" role="doc-biblioref">34</a></sup></span>: reusing the attention matrix for the last half of the network caused no degradation in performance.</p>
<center>
<div class="layout-chunk" data-layout="l-body">
<div class="figure"><span id="fig:unnamed-chunk-5"></span>
<img src="deepvit.png" alt="Attention and feature map cross layer similarity (left); ViT and DeepViT comparison (right)." width="100%" />
<p class="caption">
Figure 5: Attention and feature map cross layer similarity (left); ViT and DeepViT comparison (right).
</p>
</div>
</div>
</center>
<p>The resulting models, called Deep Vision Transformers (DeepViTs), are characterized by better performance as depth increases.</p>
<p><a href="https://arxiv.org/abs/2103.17239"><strong>Going deeper with Image Transformers</strong></a><span class="citation" data-cites="cait"><sup><a href="#ref-cait" role="doc-biblioref">35</a></sup></span> identified two main issues in DeiT models: the lack of performance improvement (and even performance degradation) at increased network depth and the double objective that characterizes the transformer encoder, which has to model both inter-patch relationships as well as that between the class token and the patch embeddings. To demonstrate the latter, the authors show that a DeiT where the class token is inserted later on in the network outperforms a normal DeiT.</p>
<p>Two main contributions characterize the paper:</p>
<ul>
<li>LayerScale, a novel normalization strategy applied to residual branches.</li>
<li>Class-attention layers, a layer dedicated to the efficient extraction of information relevant to the classification task from the processed patch embeddings.</li>
</ul>
<p>LayerScale is characterized by a layer-specific set of learnable diagonal matrices, whose diagonal values are initialized to a small <span class="math inline">\(\epsilon\)</span>, applied to the output of every residual branch. It is conceptually similar to methods like <a href="https://openreview.net/forum?id=H1gsz30cKX">FixUp</a><span class="citation" data-cites="fixup"><sup><a href="#ref-fixup" role="doc-biblioref">36</a></sup></span> and <a href="https://openreview.net/forum?id=wmqLC6f5ens">SkipInit</a><span class="citation" data-cites="skipinit"><sup><a href="#ref-skipinit" role="doc-biblioref">37</a></sup></span> but provides the model with more freedom since it is per-channel (unlike other methods that only use a single scalar).</p>
<p>As mentioned before, Class-attention layers allow efficient extraction of information from processed patch embeddings. The authors find it a better alternative than the simple late-stage insertion of the class token and global average pooling, achieving the same accuracy with lower computational costs.</p>
<center>
<div class="layout-chunk" data-layout="l-body">
<div class="figure"><span id="fig:unnamed-chunk-6"></span>
<img src="cait.png" alt="ViT (left), late class token ViT (center) and CaiT (right)." width="100%" />
<p class="caption">
Figure 6: ViT (left), late class token ViT (center) and CaiT (right).
</p>
</div>
</div>
</center>
<p>The authors also proposed the use of additional bells and whistles, such as <a href="https://arxiv.org/abs/2003.02436">Talking Heads Attention</a><span class="citation" data-cites="talking"><sup><a href="#ref-talking" role="doc-biblioref">38</a></sup></span>.</p>
<p>The resulting models, called Class-attention image Transformers (CaiTs), achieved notable performance on the ImageNet benchmark. CaiTs even outperform the recent <a href="https://arxiv.org/abs/2102.06171">NFNets</a><span class="citation" data-cites="nfnet"><sup><a href="#ref-nfnet" role="doc-biblioref">39</a></sup></span> in both the accuracy/parameters trade-off and the accuracy/FLOPs trade-off when using both the DeiT training recipe as well as the DeiT distillation technique.</p>
<h3 id="convolutional-insights">Convolutional insights</h3>
<p><a href="https://arxiv.org/abs/2103.10697"><strong>ConViT: Improving Vision Transformers with Soft Convolutional Inductive Biases</strong></a><span class="citation" data-cites="convit"><sup><a href="#ref-convit" role="doc-biblioref">40</a></sup></span> investigated the exciting possibility of initializing self-attention blocks with soft convolutional biases. Basing their work on studies regarding the <a href="https://openreview.net/forum?id=HJlnC1rKPB">theoretical relationship between self-attention and convolutional layers</a><span class="citation" data-cites="relationship"><sup><a href="#ref-relationship" role="doc-biblioref">41</a></sup></span>, the authors introduced Gated Positional Self-Attention (GPSA), a variant of self-attention which is characterized by the possibility of being initialized with a locality bias.</p>
<p>More precisely, the initialization of a GPSA block is parameterized by a head-specific center of attention (the position to which the head pays most attention to, given the query patch) and a locality strength (which determines how focused every head is around its center of attention). GPSA blocks also employ a gating mechanism to better balance content and positional information. By appropriately setting the centers of attention and the locality strength of GPSA blocks, the model can compete with CNNs in the low data regime and at the same time enjoy ViT-like expressive power in large data regimes.</p>
<div class="layout-chunk" data-layout="l-body">
<div class="figure"><span id="fig:unnamed-chunk-7"></span>
<img src="convit.png" alt="Input image (top left), attention maps of an untrained SA block (top right) and GPSA blocks (center and bottom)." width="100%" />
<p class="caption">
Figure 7: Input image (top left), attention maps of an untrained SA block (top right) and GPSA blocks (center and bottom).
</p>
</div>
</div>
<p>The authors demonstrated the effectiveness of this elegant approach through an empirical study comparing DeiTs to the resulting models, which they named Convolutional Vision Transformers (ConViTs): on ImageNet ConViTs enjoy progressively superior performance as the sample size is diminished while retaining DeiT-like performance at full sample size.</p>
<p>Two more papers exploring the application of convolutional insights to Vision Transformers are <a href="https://arxiv.org/abs/2103.11816"><strong>Incorporating Convolution Designs into Visual Transformers</strong></a><span class="citation" data-cites="ceit"><sup><a href="#ref-ceit" role="doc-biblioref">42</a></sup></span> and <a href="https://arxiv.org/abs/2104.05707"><strong>LocalViT: Bringing Locality to Vision Transformers</strong></a><span class="citation" data-cites="localvit"><sup><a href="#ref-localvit" role="doc-biblioref">43</a></sup></span>.</p>
<p>The first paper has three main contributions:</p>
<ul>
<li>The Image-to-Tokens (I2T) stem, substituting ViT’s convolutional stem and characterized by the addition of a max-pooling operation followed by batch normalization.</li>
<li>The Locally enhanced Feedforward block, substituting ViT’s feedforward block and characterized by the use of depthwise convolutions and batch normalization.</li>
<li>The Layer-wise Class-Token Attention, which is applied at the end of the network and attends unidirectionally to all class tokens throughout the network.</li>
</ul>
<p>The authors adopted the DeiT training recipe; their models, called Convolution-enhanced image Transformers (CeiTs), obtained superior results against both same-size DeiT models and same-size distilled DeiT models.</p>
<p>The second paper also studied the application of depth-wise convolutions in the feedforward networks while ablating the use of different activation functions and the application of Squeeze-and-Excite layers.</p>
<p>The authors applied this approach to several models, obtaining favorable results.</p>
<h3 id="the-importance-of-hierarchy">The importance of Hierarchy</h3>
<p>Several papers have studied the application of a hierarchical structure to Vision Transformers.</p>
<p><a href="https://arxiv.org/abs/2101.11986"><strong>Tokens-to-Token ViT: Training Vision Transformers from Scratch on ImageNet</strong></a><span class="citation" data-cites="t2t"><sup><a href="#ref-t2t" role="doc-biblioref">44</a></sup></span> introduced the Token-to-Token module (T2T), a module that reshapes the input sequence to a 2D structure, applies a soft split (allowing overlapping patches), and flattens the resulting patches. By adjusting the patch size used in the module, the length of the token sequences diminishes progressively throughout the network.</p>
<div class="layout-chunk" data-layout="l-body">
<div class="figure" style="text-align: center"><span id="fig:unnamed-chunk-8"></span>
<img src="t2t.png" alt="The Token-to-Token module architecture." width="100%" />
<p class="caption">
Figure 8: The Token-to-Token module architecture.
</p>
</div>
</div>
<p>The authors obtained favorable results, with performance comparable to that of <a href="https://ieeexplore.ieee.org/document/8578572">MobileNets</a><span class="citation" data-cites="mobile"><sup><a href="#ref-mobile" role="doc-biblioref">45</a></sup></span>.</p>
<p><a href="https://arxiv.org/abs/2102.12122"><strong>Pyramid Vision Transformer: A Versatile Backbone for Dense Prediction without Convolutions</strong></a><span class="citation" data-cites="pvt"><sup><a href="#ref-pvt" role="doc-biblioref">46</a></sup></span> introduced a variant of Self-Attention called spatial-reduction attention (SRA), characterized by spatial reduction of both keys and values. By applying SRA at the end of several stages, the spatial dimensions of the feature map slowly decrease throughout the model. The resulting models, called Pyramid Vision Transformers (PVTs), can deal with a variety of tasks, including dense prediction, object detection, and semantic segmentation, where high-resolution images are typical.</p>
<div class="layout-chunk" data-layout="l-body">
<div class="figure" style="text-align: center"><span id="fig:unnamed-chunk-9"></span>
<img src="pvt.png" alt="The Pyramid Vision Transformer." width="100%" />
<p class="caption">
Figure 9: The Pyramid Vision Transformer.
</p>
</div>
</div>
<p><a href="https://arxiv.org/abs/2103.10619"><strong>Scalable Visual Transformers with Hierarchical Pooling</strong></a><span class="citation" data-cites="hvt"><sup><a href="#ref-hvt" role="doc-biblioref">47</a></sup></span> explored the use of max-pooling to diminish the sequence length progressively. The authors also replaced the class token used in ViT with a final global average pooling layer. The resulting models, called Hierarchical Vision Transformers (HVTs), are trained with the DeiT recipe and scaled up (in the embedding dimension and number of heads) to have a comparable computational cost to DeiTs, achieving gains in the low (sub <span class="math inline">\(5\)</span>) GFLOPs regime.</p>
<p><a href="https://arxiv.org/abs/2103.14030"><strong>Swin Transformer: Hierarchical Vision Transformer using Shifted Windows</strong></a><span class="citation" data-cites="swin"><sup><a href="#ref-swin" role="doc-biblioref">48</a></sup></span> suggested a different route: use local self-attention inside (non-overlapping, local) windows, allow cross-window communication through so-called <em>shifted window partitioning</em>, and produce a hierarchical representation by progressively merging the windows themselves.</p>
<div class="layout-chunk" data-layout="l-body">
<div class="figure" style="text-align: center"><span id="fig:unnamed-chunk-10"></span>
<img src="swin2.png" alt="The Swin Transformer shifted window partitioning procedure." width="100%" />
<p class="caption">
Figure 10: The Swin Transformer shifted window partitioning procedure.
</p>
</div>
</div>
<p>In the last stage of the network, all local windows have been merged, resulting in blocks effectively using global self-attention on a feature map whose spatial dimensions have been significantly decreased. It’s worth noting that this approach scales linearly with image size.</p>
<div class="layout-chunk" data-layout="l-body">
<div class="figure" style="text-align: center"><span id="fig:unnamed-chunk-11"></span>
<img src="swin1.png" alt="Swin Transformer (left) and ViT (right). Attention is applied on patches (black) inside windows (red)." width="100%" />
<p class="caption">
Figure 11: Swin Transformer (left) and ViT (right). Attention is applied on patches (black) inside windows (red).
</p>
</div>
</div>
<p>The authors also reported positive results applying T5-style relative positional bias in attention blocks and obtained promising results on both ImageNet and ImageNet-21k, as well as in object detection and semantic segmentation tasks.</p>
<p><a href="https://arxiv.org/abs/2103.16302"><strong>Rethinking Spatial Dimensions of Vision Transformers</strong></a><span class="citation" data-cites="pit"><sup><a href="#ref-pit" role="doc-biblioref">49</a></sup></span> introduced a novel pooling layer, characterized by a depthwise convolution (for patch embeddings) and a fully connected layer (for the class token). This simple change allowed the models, named Pooling-based Vision Transformers (PiTs), to outperform vanilla Vision Transformers in the ImageNet data regime.</p>
<p><a href="https://arxiv.org/abs/2104.01136"><strong>LeViT: a Vision Transformer in ConvNet’s Clothing for Faster Inference</strong></a><span class="citation" data-cites="levit"><sup><a href="#ref-levit" role="doc-biblioref">50</a></sup></span> studied a hybrid architecture characterized by a longer convolutional stem and shrinking attention blocks that progressively diminish the spatial dimensions of the feature maps throughout the network. The authors also proposed the use of global average pooling instead of the class token, the injection of positional information through attention bias, and the addition of a <a href="https://arxiv.org/abs/1606.08415">GELU activation</a><span class="citation" data-cites="gelu"><sup><a href="#ref-gelu" role="doc-biblioref">51</a></sup></span> in the attention block.</p>
<p>The authors named the models LeViTs, trained them using both the DeiT training recipe and the DeiT distillation procedure, and showed that the networks are capable of high-speed inference, outperforming EfficientNets and DeiTs in the accuracy/step-time trade-off, on both GPU and CPU.</p>
<h3 id="multi-scale-features-and-cross-attention">Multi-Scale features and Cross-Attention</h3>
<p><a href="https://arxiv.org/abs/2103.14899"><strong>CrossViT: Cross-Attention Multi-Scale Vision Transformer for Image Classification</strong></a><span class="citation" data-cites="crossvit"><sup><a href="#ref-crossvit" role="doc-biblioref">52</a></sup></span> proposed the use of multi-scale features by adapting the Vision Transformer to have two branches:</p>
<ul>
<li>A large (or primary) one characterized by large patch size, deep transformer encoder, and wide embedding dimension.</li>
<li>A small (or complementary) one characterized by smaller patch size, shallower encoder, and narrower embedding dimension.</li>
</ul>
<p>The branches employ two separate class tokens. Late in the network (after a separate set of positional embeddings is added), cross-branch communication is established through the use of Cross-Attention Fusion blocks.</p>
<p>In particular, inside cross-attention fusion blocks, class tokens are concatenated to the patch embeddings of the other branch and processed by attention before being returned to their respective branch.</p>
<div class="layout-chunk" data-layout="l-body">
<div class="figure" style="text-align: center"><span id="fig:unnamed-chunk-12"></span>
<img src="crossvit.png" alt="The Cross-Attention fusion layer for the large branch." width="100%" />
<p class="caption">
Figure 12: The Cross-Attention fusion layer for the large branch.
</p>
</div>
</div>
<p>The output of MLP heads based on the large and small branches tokens is then added together to generate the model’s logits.</p>
<p>The resulting models, which the authors named CrossViTs, are trained with the DeiT recipe and enjoy significant performance boosts, achieving better performance than DeiTs twice as large and twice as computationally expensive.</p>
<h3 id="integrating-convolutions-in-attention">Integrating convolutions in attention</h3>
<p><a href="https://arxiv.org/abs/2103.15808"><strong>CvT: Introducing Convolutions to Vision Transformers</strong></a><span class="citation" data-cites="cvt"><sup><a href="#ref-cvt" role="doc-biblioref">53</a></sup></span> can be seen as a complementary approach to Bottleneck Transformers, where instead of using multi-head self-attention inside of a CNN’s final blocks, convolutions (in this case, <a href="https://arxiv.org/abs/1610.02357">depthwise separable ones</a><span class="citation" data-cites="depthwise"><sup><a href="#ref-depthwise" role="doc-biblioref">54</a></sup></span>) are used inside a Vision Transformer’s self-attention blocks. More precisely, Convolutional vision Transformers (CvTs, whose full-name conflicts with ConViTs) are characterized by two main features:</p>
<ul>
<li>The Convolutional Token Embedding, a module characterized by a strided convolution and inserted at the beginning of every stage.</li>
<li>The Convolutional Projection(s) in the attention blocks. These projections, implemented through depthwise separable convolutions, allow queries, keys, and values to be influenced by neighboring tokens. Further, larger strides for keys and values diminish the tokens spatial dimensions, decreasing the associated parameters count and computational cost.</li>
</ul>
<div class="layout-chunk" data-layout="l-body">
<div class="figure" style="text-align: center"><span id="fig:unnamed-chunk-13"></span>
<img src="cvt.png" alt="The (strided) Convolutional Projections." width="100%" />
<p class="caption">
Figure 13: The (strided) Convolutional Projections.
</p>
</div>
</div>
<p>These two features allow the model to progressively reduce the tokens feature and spatial dimensions, allowing CvTs to adopt a hierarchical multi-stage architecture.</p>
<p>It’s worth noting that CvTs do not employ positional embeddings since positional information is retained through the use of convolutions both in the embedding layers as well as in the attention blocks.</p>
<p>The authors adopted the original ViT training recipe and achieved competitive performance when training CvTs on ImageNet. The largest model presented (CvT-W24), once pre-trained on ImageNet-21k, obtained a stunning <span class="math inline">\(87.7\%\)</span> top-1 accuracy, outperforming BiT-L (an ensemble of CNNs pre-trained on <span class="math inline">\(20\)</span> times more data) with a fraction of parameters and compute.</p>
<p><a href="https://arxiv.org/abs/2103.15358"><strong>Multi-Scale Vision Longformer: A New Vision Transformer for High-Resolution Image Encoding</strong></a><span class="citation" data-cites="vil"><sup><a href="#ref-vil" role="doc-biblioref">55</a></sup></span> introduced a 2D version of <a href="https://arxiv.org/abs/2004.05150">Longformer</a><span class="citation" data-cites="long"><sup><a href="#ref-long" role="doc-biblioref">56</a></sup></span>, which the authors call Vision Longformer. Conceptually, it is characterized by two different sets of tokens: a set of global tokens, allowed to attend to all tokens, and local tokens, only allowed to attend to global tokens and tokens spatially close to them.</p>
<div class="layout-chunk" data-layout="l-body">
<div class="figure" style="text-align: center"><span id="fig:unnamed-chunk-14"></span>
<img src="vil.png" alt="Global and local tokens in Multi-Scale Vision Longformers." width="100%" />
<p class="caption">
Figure 14: Global and local tokens in Multi-Scale Vision Longformers.
</p>
</div>
</div>
<p>It’s worth noting that, at least in vision-only tasks, global tokens are discarded at the end of every attention block (while local tokens are reshaped and passed to the next one), since they have at that point already fulfilled their role of allowing efficient communication between spatially distant tokens.</p>
<p>The authors adopted DeiT-style training and achieved impressive parameter and FLOP efficiency.</p>
<h3 id="haloing-and-strided-local-self-attention">Haloing and Strided local Self-Attention</h3>
<p><a href="https://arxiv.org/abs/2103.12731"><strong>Scaling Local Self-Attention for Parameter Efficient Visual Backbones</strong></a><span class="citation" data-cites="halonet"><sup><a href="#ref-halonet" role="doc-biblioref">57</a></sup></span> builds on top of SASA models, by developing a local self-attention-only family of models made more efficient by the newly introduced haloing operation.</p>
<p>More precisely, the authors introduced a novel strategy called Blocked Self-Attention: input is first divided into blocks (which will be used as queries), and neighboring pixels are banded together and padded, in an operation denominated haloing, to generate keys and values. Finally, attention is applied.</p>
<div class="layout-chunk" data-layout="l-body">
<div class="figure" style="text-align: center"><span id="fig:unnamed-chunk-15"></span>
<img src="halonet.png" alt="Blocked local Self-Attention as used in HaloNets." width="100%" />
<p class="caption">
Figure 15: Blocked local Self-Attention as used in HaloNets.
</p>
</div>
</div>
<p>It’s worth noting that this operation causes layers not to be translationally equivariant, but the authors took this route to obtain higher hardware utilization.</p>
<p>The network assumes a hierarchical structure thanks to a strided version of self-attention that is applied at the end of every stage (thus replacing SASA models post-attention average pooling).</p>
<p>The resulting models, called HaloNets, achieved extremely high parameter efficiency, slightly surpassing EfficientNets, a feat not obtained by any other model thus far. It’s worth, however, noting that HaloNets have a longer step-time, particularly when using larger configurations.</p>
<h3 id="second-order-pooling">Second-order pooling</h3>
<p><a href="https://arxiv.org/abs/2104.10935"><strong>So-ViT: Mind Visual Tokens for Vision Transformer</strong></a><span class="citation" data-cites="sovit"><sup><a href="#ref-sovit" role="doc-biblioref">58</a></sup></span> recently proposed the use of second-order pooling to extract high-level information from visual tokens (that is all tokens apart from the class one). The model’s logits are finally obtained by summing up the output of two separate linear heads, one applied to the class token, and one applied to the pooled features.</p>
<div class="layout-chunk" data-layout="l-body">
<div class="figure" style="text-align: center"><span id="fig:unnamed-chunk-16"></span>
<img src="sovit.png" alt="The So-ViT architecture." width="100%" />
<p class="caption">
Figure 16: The So-ViT architecture.
</p>
</div>
</div>
<p>The resulting models, named Second-order ViTs (So-ViTs), are trained with an expanded DeiT recipe, and obtain competitive results.</p>
<h2 id="a-comparison-for-image-classification">A Comparison for Image Classification</h2>
<p>Before continuing, we recap the reported performance of most models presented up to this point.</p>
<p>To provide a fair comparison, we only consider models trained at the same resolution (<span class="math inline">\(224 \times 224\)</span>) apart from ViTs that were trained at <span class="math inline">\(384 \times 384\)</span>. We also denote models fine-tuned at higher (<span class="math inline">\(384\times384\)</span>) resolution with an upward arrow, and exclude models trained for more than <span class="math inline">\(400\)</span> epochs or using the DeiT hard-label distillation technique, because only the DeiT, T2T, and CaiT papers report results using it. Some convolutional models (EfficientNets, NFNets, ResNet-RS and EfficientNetV2s) are included for reference, even though they use both different recipes and training resolution.</p>
<p>We follow <a href="https://openreview.net/forum?id=Bygq-H9eg"><strong>An Analysis of Deep Neural Network Models for Practical Applications</strong></a><span class="citation" data-cites="canziani"><sup><a href="#ref-canziani" role="doc-biblioref">59</a></sup></span> in graphing ImageNet top-1 accuracy vs. FLOPs and parameter count. A lot of models are displayed, so make sure to enable and disable the display of the various models (by clicking them in the legend) as to better explore the visualization.</p>
<div class="layout-chunk" data-layout="l-page">
<p><iframe src="imagenet1k.html" width="100%" height="640" scrolling="no" seamless="seamless" frameBorder="0"></iframe></p>
</div>
<div class="layout-chunk" data-layout="l-page">
<p><iframe src="imagenet21k.html" width="100%" height="640" scrolling="no" seamless="seamless" frameBorder="0"></iframe></p>
</div>
<p>It’s worth keeping in mind that FLOP use and parameter count are not necessarily representative of latency or memory consumption.</p>
<p>As the recent <a href="https://arxiv.org/abs/2103.07579">ResNet-RS paper</a><span class="citation" data-cites="resnetrs"><sup><a href="#ref-resnetrs" role="doc-biblioref">60</a></sup></span> explains, <em>“in custom hardware architectures (e.g. TPUs and GPUs), operations are often bounded by memory access costs and have different levels of optimization on modern matrix multiplication units.”</em> For these reasons, FLOPs are a particularly poor proxy for latency time.</p>
<p>Similarly, the number of parameters is a poor indicator of memory consumption during training. Again from the ResNet-RS paper, <em>“parameter count does not necessarily dictate memory consumption during training because memory is often dominated by the size of the activations”</em> which have to be stored to execute the backpropagation algorithm. <em>“At inference, activations can be discarded and parameter count is a better proxy for actual memory consumption.”</em></p>
<p>ResNet-RS models are a great example of this issue: an ResNet-RS has respectively <span class="math inline">\(3{-}4\times\)</span> and <span class="math inline">\(2\times\)</span> the amount of parameters and FLOPs of a similarly accurate EfficientNet, yet it is <span class="math inline">\(3\times\)</span> as fast and consumes about <span class="math inline">\(2\times\)</span> less memory. For these reasons, it would be preferable for authors of new studies to include, together with parameter count and FLOP use, also latency and memory usage measurements.</p>
<h2 id="robustness-and-equivariance">Robustness and Equivariance</h2>
<p><a href="https://arxiv.org/abs/2103.14586"><strong>Understanding Robustness of Transformers for Image Classification</strong></a><span class="citation" data-cites="robustness"><sup><a href="#ref-robustness" role="doc-biblioref">61</a></sup></span> studied the robustness of Vision Transformers to input, model, and adversarial perturbations. The authors found Vision Transformers to have an advantage compared to CNNs of similar size for both input and model perturbations (particularly in large data regimes). However, for some kinds of adversarial attacks, Vision Transformers were found to be more susceptible, at least for small sample sizes.</p>
<p>The authors also found vanilla Vision Transformers highly redundant, a finding first reported in DeepViTs, suggesting the possible use of heavy pruning as a path to more efficient models.</p>
<p>In <a href="https://openreview.net/forum?id=JkfYjnOEo6M"><strong>Group Equivariant Stand-Alone Self-Attention For Vision</strong></a><span class="citation" data-cites="gsa"><sup><a href="#ref-gsa" role="doc-biblioref">62</a></sup></span>, the authors developed a self-attention formulation equivariant to arbitrary symmetry groups. The resulting models, called Group Self-Attention Networks (GSA-Nets), enjoy superior parameter efficiency, although not quite reaching that of comparable-size equivariant CNNs.</p>
<h2 id="vision-transformers-in-self-supervised-learning">Vision Transformers in Self-Supervised Learning</h2>
<p>Building on top of previous studies (<a href="https://openai.com/blog/image-gpt/">iGPT</a><span class="citation" data-cites="igpt"><sup><a href="#ref-igpt" role="doc-biblioref">63</a></sup></span> and <a href="https://openreview.net/pdf?id=YicbFdNTTy#subsubsection.B.1.2">masked patch prediction in the Vision Transformers paper</a><span class="citation" data-cites="vit"><sup><a href="#ref-vit" role="doc-biblioref">19</a></sup></span>), two papers have investigated self-supervised approaches using Vision Transformers.</p>
<p><a href="https://arxiv.org/abs/2104.02057"><strong>An Empirical Study of Training Self-Supervised Vision Transformers</strong></a><span class="citation" data-cites="random"><sup><a href="#ref-random" role="doc-biblioref">64</a></sup></span> identified an instability encountered during self-supervised training, causing performance degradation, particularly at large batch sizes. The authors propose a curious trick: a fixed, random patch projection, unlike normal ViTs where patch embeddings are learned just like the rest of the network.</p>
<div class="layout-chunk" data-layout="l-body">
<div class="figure" style="text-align: center"><span id="fig:unnamed-chunk-19"></span>
<img src="random.png" alt="A comparison of training stability using learned and random patch embeddings." width="100%" />
<p class="caption">
Figure 17: A comparison of training stability using learned and random patch embeddings.
</p>
</div>
</div>
<p>The authors showed that, by coupling this trick with the use of batch normalization in MLP blocks (instead of Layer Normalization), self-supervised training of Vision Transformers could be made more stable, allowing the models to achieve superior performance on several downstream tasks.</p>
<p>Unfortunately, the authors also noted that this approach was not sufficient: the instability is alleviated but not solved, particularly at large learning rates.</p>
<p><a href="https://arxiv.org/abs/2104.03602"><strong>SiT: Self-supervised vIsion Transformer</strong></a><span class="citation" data-cites="sit"><sup><a href="#ref-sit" role="doc-biblioref">65</a></sup></span>, studied a mix of three semi-supervised tasks (rotation prediction, image reconstruction, and a contrastive one) that, when combined, allow Vision Transformers to perform well on downstream tasks.</p>
<div class="layout-chunk" data-layout="l-body">
<div class="figure" style="text-align: center"><span id="fig:unnamed-chunk-20"></span>
<img src="sit.png" alt="The Self-supervised vIsion Transformer architecture." width="100%" />
<p class="caption">
Figure 18: The Self-supervised vIsion Transformer architecture.
</p>
</div>
</div>
<h2 id="vision-transformers-in-medical-research">Vision Transformers in Medical Research</h2>
<p>Vision Transformers have recently been applied to medical research, mainly in segmentation tasks and diagnosis prediction ones.</p>
<p><a href="https://arxiv.org/abs/2102.04306"><strong>TransUNet: Transformers Make Strong Encoders for Medical Image Segmentation</strong></a><span class="citation" data-cites="transunet"><sup><a href="#ref-transunet" role="doc-biblioref">66</a></sup></span> explored using a hybrid CNN-ViT encoder in an UNet-style architecture they call TransUNet. The authors reported favorable results in several segmentation tasks, especially when using a cascaded upsampler to obtain the final segmentation mask.</p>
<p>In <a href="https://arxiv.org/abs/2102.10662"><strong>Medical Transformer: Gated Axial-Attention for Medical Image Segmentation</strong></a><span class="citation" data-cites="medt"><sup><a href="#ref-medt" role="doc-biblioref">67</a></sup></span>, the authors proposed the use of two branches, a shallower one, operating on the global context of the image, and a deeper one, operating locally on patches. The basic building block used for the model, which they named Medical Transformer (MedT), is (gated) axial self-attention<span class="citation" data-cites="axial"><sup><a href="#ref-axial" role="doc-biblioref">16</a></sup></span>. The authors obtained improvements over baselines on several medical segmentation tasks.</p>
<div class="layout-chunk" data-layout="l-body">
<div class="figure" style="text-align: center"><span id="fig:unnamed-chunk-21"></span>
<img src="medt.png" alt="The Medical Transformer architecture." width="100%" />
<p class="caption">
Figure 19: The Medical Transformer architecture.
</p>
</div>
</div>
<p><a href="https://arxiv.org/abs/2103.10504"><strong>UNETR: Transformers for 3D Medical Image Segmentation</strong></a><span class="citation" data-cites="unetr"><sup><a href="#ref-unetr" role="doc-biblioref">68</a></sup></span> adapted ViTs for 3D medical segmentation tasks. The authors showed that a simple adaptation was sufficient to improve over baselines on several 3D segmentation tasks.</p>
<h2 id="vision-transformers-for-video-recognition">Vision Transformers for Video Recognition</h2>
<p><a href="https://arxiv.org/abs/2102.00719"><strong>Video Transformer Network</strong></a><span class="citation" data-cites="vtn"><sup><a href="#ref-vtn" role="doc-biblioref">69</a></sup></span> (VTN) proposed the use of a pre-trained 2D spatial backbone (the authors experiment with both CNN-based and ViT-based feature extractors) combined with a Transformer (in this case, a Longformer) operating on the extracted feature maps. The authors showed that this simple approach was competitive with baselines such as <a href="https://arxiv.org/abs/1812.03982">SlowFast</a><span class="citation" data-cites="slowfast"><sup><a href="#ref-slowfast" role="doc-biblioref">70</a></sup></span>.</p>
<p><a href="https://arxiv.org/abs/2102.05095"><strong>Is Space-Time Attention All You Need for Video Understanding?</strong></a><span class="citation" data-cites="timesformer"><sup><a href="#ref-timesformer" role="doc-biblioref">71</a></sup></span> introduced TimeSformer, an adaption of ViTs to video. After exploring several possible attention variants, the authors proposed one called Divided Space-Time attention. In this approach, frames are first divided into patches and linearly embedded, then fed to a single transformer encoder. Every encoder layer has two attention blocks applied consecutively: the first one on patch embeddings at the same location but in different frames (temporal attention), the second on patch embeddings in the same frame (spatial attention).</p>
<p>The authors adopted ImageNet pretraining for spatial attention blocks and showed that the resulting models achieved state of the art, outperforming previous baselines on standard datasets such as <a href="https://arxiv.org/abs/1705.06950">Kinetics-400</a><span class="citation" data-cites="kinetics"><sup><a href="#ref-kinetics" role="doc-biblioref">72</a></sup></span>.</p>
<p><a href="https://arxiv.org/abs/2103.13915"><strong>An Image is Worth 16x16 Words, What is a Video Worth?</strong></a><span class="citation" data-cites="videoworth"><sup><a href="#ref-videoworth" role="doc-biblioref">73</a></sup></span> proposed using a Vision Transformer to model the relationship between intra-frame patches, followed by a (temporal) Transformer encoder, modeling inter-frame relationships. It is conceptually similar to a VTN with a ViT-based spatial backbone.</p>
<p>The resulting models (called Spatio and Temporal Transformers, or STAMs) outperformed strong baselines such as <a href="https://arxiv.org/abs/2004.04730">X3D</a><span class="citation" data-cites="x3d"><sup><a href="#ref-x3d" role="doc-biblioref">74</a></sup></span> in the accuracy/FLOPs trade-off.</p>
<p><a href="https://arxiv.org/abs/2103.15691"><strong>ViViT: A Video Vision Transformer</strong></a><span class="citation" data-cites="vivit"><sup><a href="#ref-vivit" role="doc-biblioref">75</a></sup></span> discusses several approaches to adapt ViTs to video, and found the use of <em>tubelet embeddings</em>, linear projections of spatio-temporal tubes, to be most effective.</p>
<div class="layout-chunk" data-layout="l-body">
<div class="figure" style="text-align: center"><span id="fig:unnamed-chunk-22"></span>
<img src="vivit.png" alt="Tubelet embeddings in ViViT." width="100%" />
<p class="caption">
Figure 20: Tubelet embeddings in ViViT.
</p>
</div>
</div>
<p>The positional embeddings are initialized by temporally repeating positional embeddings obtained from pre-trained ViTs. Tubelet embeddings are instead initialized using “central frame initialization”, where the temporally central frame’s embedding is initialized using pre-trained ViT patch embeddings, while the rest are initialized with zeros.</p>
<p>The resulting models, called Video Vision Transformers (ViViTs), achieved state of the art, outperforming TimeSformers, on several standard benchmarks. The authors even reported experiments using an extremely large backbone called ViViT-H, initialized from the similarly large ViT-H pre-trained on JFT-300M. The model obtained a substantial performance improvement, albeit with significant computational cost.</p>
<p>Recently, <a href="https://arxiv.org/abs/2104.11227"><strong>Multiscale Vision Transformers</strong></a><span class="citation" data-cites="mvit"><sup><a href="#ref-mvit" role="doc-biblioref">76</a></sup></span> proposed the adoption of a hierarchical structure by progressively increasing the feature dimension while decreasing the spatiotemporal one. This is achieved through the use of pooling, applied to keys and values in attention blocks.</p>
<p>The authors demonstrated the effectiveness of this approach by showing how the resulting models, called Multiscale Vision Transformers (MViTs), could outperform TimeSFormers and ViViTs on standard benchmarks while using a fraction of the compute. Importantly, this method does not require additional data (unlike TimeSFormers and ViViTs, that required ImageNet pretraining).</p>
<h2 id="conclusion">Conclusion</h2>
<p>At the time of writing, the only convolutional architecture still unsurpassed by transformers in efficiency is the recent <a href="https://arxiv.org/abs/2104.00298">EfficientNetV2 family of models</a><span class="citation" data-cites="efficientnetv2"><sup><a href="#ref-efficientnetv2" role="doc-biblioref">77</a></sup></span>, obtained through extensive neural architecture search. No similarly extensive search has been performed for ViT-based models, but the small one reported in <a href="https://arxiv.org/abs/2103.15808"><strong>CvT: Introducing Convolutions to Vision Transformers</strong></a><span class="citation" data-cites="cvt"><sup><a href="#ref-cvt" role="doc-biblioref">53</a></sup></span> achieved promising results (CvT-13-NAS).</p>
<p>From recent research, we can draw several conclusions:</p>
<ul>
<li>Inductive biases shape a model’s performance. The hard inductive biases that characterize convolutional neural networks provide them with a higher performance floor and a lower performance ceiling. Conversely, transformer-based approaches struggle to compete in the small data regime (thus requiring stronger data augmentations) but shine in regimes where data is plentiful. As many papers investigate, an effective solution may be injecting soft inductive biases in transformers, achieving both high sample efficiency in small data regimes and high model expressiveness when sufficient data is provided.</li>
<li>Vanilla Vision Transformers appear highly redundant; be it through custom normalization strategies, hierarchical structure, or pruning, they can be made more computationally efficient at little or no performance cost.</li>
<li>Several studies have highlighted issues with the use of the class token in vanilla Vision Transformers. Better alternatives appear to be both late stage insertion of the class token and the use of global average pooling. Second-order pooling and class-attention layers seem to be better still.</li>
<li>The amount of research coming out on a daily basis is significant. For this reason, we would like to highlight a few models for a more in-depth look, including the original <a href="https://openreview.net/forum?id=YicbFdNTTy">Vision Transformers</a><span class="citation" data-cites="vit"><sup><a href="#ref-vit" role="doc-biblioref">19</a></sup></span>, <a href="https://arxiv.org/abs/2012.12877">DeiTs</a><span class="citation" data-cites="deit"><sup><a href="#ref-deit" role="doc-biblioref">20</a></sup></span>, <a href="https://arxiv.org/abs/2103.00112">TNTs</a><span class="citation" data-cites="tnt"><sup><a href="#ref-tnt" role="doc-biblioref">32</a></sup></span>, <a href="https://arxiv.org/abs/2103.10697">ConViTs</a><span class="citation" data-cites="convit"><sup><a href="#ref-convit" role="doc-biblioref">40</a></sup></span>, <a href="https://arxiv.org/abs/2103.14030">Swin Transformers</a><span class="citation" data-cites="swin"><sup><a href="#ref-swin" role="doc-biblioref">48</a></sup></span>, <a href="https://arxiv.org/abs/2103.15808">CvTs</a><span class="citation" data-cites="cvt"><sup><a href="#ref-cvt" role="doc-biblioref">53</a></sup></span>, <a href="https://arxiv.org/abs/2103.17239">CaiTs</a><span class="citation" data-cites="cait"><sup><a href="#ref-cait" role="doc-biblioref">35</a></sup></span>, and <a href="https://arxiv.org/abs/2103.15358">ViLs</a><span class="citation" data-cites="vil"><sup><a href="#ref-vil" role="doc-biblioref">55</a></sup></span>.</li>
<li>Transformers in vision are just getting started, and their performance is likely to continue improving as a wider part of the computer vision community adopts them. Increases in compute and data availability are also likely to increasingly tilt the balance in their favor.</li>
</ul>
<p>The success of transformers in vision has had far-reaching consequences. We have discussed their influence on medical and video tasks, but their impact goes as far as audio (with <a href="https://arxiv.org/abs/2104.01778">AST</a><span class="citation" data-cites="ast"><sup><a href="#ref-ast" role="doc-biblioref">78</a></sup></span>, <a href="https://arxiv.org/abs/2104.00769">Keyword Transformer</a><span class="citation" data-cites="keyword"><sup><a href="#ref-keyword" role="doc-biblioref">79</a></sup></span>) and multimodal architectures (such as <a href="https://arxiv.org/abs/2102.03334">ViLT</a><span class="citation" data-cites="vilt"><sup><a href="#ref-vilt" role="doc-biblioref">80</a></sup></span> and <a href="https://arxiv.org/abs/2104.11178">VATT</a><span class="citation" data-cites="vatt"><sup><a href="#ref-vatt" role="doc-biblioref">81</a></sup></span>). Their success has also inspired architectures with even fewer inductive biases, such as the <a href="https://arxiv.org/abs/2103.03206">Perceiver</a><span class="citation" data-cites="perceiver"><sup><a href="#ref-perceiver" role="doc-biblioref">82</a></sup></span>.</p>
<p>One cannot help but feel excited as more and more researchers from all over the artificial intelligence field converge toward common, more general architectures.</p>
<h2 class="appendix" id="acknowledgements">Acknowledgements</h2>
<p>Thanks to <a href="https://twitter.com/s_scardapane">Simone Scardapane</a>, <a href="https://twitter.com/unsorsodicorda">Andrea Panizza</a>, <a href="https://twitter.com/linker81">Amedeo Buonanno</a> and <a href="https://twitter.com/iacopo_poli">Iacopo Poli</a> for help in proofreading and improving this work.</p>
<div class="sourceCode" id="cb1"><pre class="sourceCode r distill-force-highlighting-css"><code class="sourceCode r"></code></pre></div>
<div id="refs" class="references csl-bib-body" data-entry-spacing="4" data-line-spacing="4" role="doc-bibliography">
<div id="ref-transformer" class="csl-entry" role="doc-biblioentry">
<div class="csl-left-margin"><strong>1. Attention is all you need</strong>. </div><div class="csl-right-inline">Vaswani, A., Shazeer, N., Parmar, N., <em>et al.</em>, 2017. </div>
</div>
<div id="ref-bert" class="csl-entry" role="doc-biblioentry">
<div class="csl-left-margin"><strong>2. BERT: Pre-training of deep bidirectional transformers for language understanding</strong>. </div><div class="csl-right-inline">Devlin, J., Chang, M.-W., Lee, K. &amp; Toutanova, K., 2019. </div>
</div>
<div id="ref-megatron" class="csl-entry" role="doc-biblioentry">
<div class="csl-left-margin"><strong>3. Megatron-LM: Training multi-billion parameter language models using model parallelism</strong>. </div><div class="csl-right-inline">Shoeybi, M., Patwary, M., Puri, R., LeGresley, P., Casper, J. &amp; Catanzaro, B., 2019. </div>
</div>
<div id="ref-t5" class="csl-entry" role="doc-biblioentry">
<div class="csl-left-margin"><strong>4. Exploring the limits of transfer learning with a unified text-to-text transformer</strong>. </div><div class="csl-right-inline">Raffel, C., Shazeer, N., Roberts, A., <em>et al.</em>, 2020. </div>
</div>
<div id="ref-gpt" class="csl-entry" role="doc-biblioentry">
<div class="csl-left-margin"><strong>5. Improving language understanding with unsupervised learning</strong>. </div><div class="csl-right-inline">Radford, A., 2019. </div>
</div>
<div id="ref-gpt2" class="csl-entry" role="doc-biblioentry">
<div class="csl-left-margin"><strong>6. Better language models and their implications</strong>. </div><div class="csl-right-inline">Radford, A., 2018. </div>
</div>
<div id="ref-gpt3" class="csl-entry" role="doc-biblioentry">
<div class="csl-left-margin"><strong>7. Language models are few-shot learners</strong>. </div><div class="csl-right-inline">Brown, T. B., Mann, B., Ryder, N., <em>et al.</em>, 2020. </div>
</div>
<div id="ref-neural_laws1" class="csl-entry" role="doc-biblioentry">
<div class="csl-left-margin"><strong>8. Scaling laws for neural language models</strong>. </div><div class="csl-right-inline">Kaplan, J., McCandlish, S., Henighan, T., <em>et al.</em>, 2020. </div>
</div>
<div id="ref-neural_laws2" class="csl-entry" role="doc-biblioentry">
<div class="csl-left-margin"><strong>9. Scaling laws for transfer</strong>. </div><div class="csl-right-inline">Hernandez, D., Kaplan, J., Henighan, T. &amp; McCandlish, S., 2021. </div>
</div>
<div id="ref-aacn" class="csl-entry" role="doc-biblioentry">
<div class="csl-left-margin"><strong>10. Attention augmented convolutional networks</strong>. </div><div class="csl-right-inline">Bello, I., Zoph, B., Le, Q., Vaswani, A. &amp; Shlens, J., 2019. </div>
</div>
<div id="ref-sasa" class="csl-entry" role="doc-biblioentry">
<div class="csl-left-margin"><strong>11. Stand-alone self-attention in vision models</strong>. </div><div class="csl-right-inline">Parmar, N., Ramachandran, P., Vaswani, A., Bello, I., Levskaya, A. &amp; Shlens, J., 2019. </div>
</div>
<div id="ref-detr" class="csl-entry" role="doc-biblioentry">
<div class="csl-left-margin"><strong>12. End-to-end object detection with transformers</strong>. </div><div class="csl-right-inline">Carion, N., Massa, F., Synnaeve, G., Usunier, N., Kirillov, A. &amp; Zagoruyko, S., 2020. </div>
</div>
<div id="ref-visual_transformer" class="csl-entry" role="doc-biblioentry">
<div class="csl-left-margin"><strong>13. Visual transformers: Token-based image representation and processing for computer vision</strong>. </div><div class="csl-right-inline">Wu, B., Xu, C., Dai, X., <em>et al.</em>, 2020. </div>
</div>
<div id="ref-lambda" class="csl-entry" role="doc-biblioentry">
<div class="csl-left-margin"><strong>14. LambdaNetworks: Modeling long-range interactions without attention</strong>. </div><div class="csl-right-inline">Bello, I., 2021. </div>
</div>
<div id="ref-image_transformer" class="csl-entry" role="doc-biblioentry">
<div class="csl-left-margin"><strong>15. Image transformer</strong>. </div><div class="csl-right-inline">Parmar, N., Vaswani, A., Uszkoreit, J., <em>et al.</em>, 2018. </div>
</div>
<div id="ref-axial" class="csl-entry" role="doc-biblioentry">
<div class="csl-left-margin"><strong>16. Axial attention in multidimensional transformers</strong>. </div><div class="csl-right-inline">Ho, J., Kalchbrenner, N., Weissenborn, D. &amp; Salimans, T., 2019. </div>
</div>
<div id="ref-survey1" class="csl-entry" role="doc-biblioentry">
<div class="csl-left-margin"><strong>17. A survey on visual transformer</strong>. </div><div class="csl-right-inline">Han, K., Wang, Y., Chen, H., <em>et al.</em>, 2020. </div>
</div>
<div id="ref-survey2" class="csl-entry" role="doc-biblioentry">
<div class="csl-left-margin"><strong>18. Transformers in vision: <span>A</span> survey</strong>. </div><div class="csl-right-inline">Khan, S., Naseer, M., Hayat, M., Zamir, S. W., Khan, F. S. &amp; Shah, M., 2021. </div>
</div>
<div id="ref-vit" class="csl-entry" role="doc-biblioentry">
<div class="csl-left-margin"><strong>19. An image is worth 16x16 words: Transformers for image recognition at scale</strong>. </div><div class="csl-right-inline">Dosovitskiy, A., Beyer, L., Kolesnikov, A., <em>et al.</em>, 2021. </div>
</div>
<div id="ref-deit" class="csl-entry" role="doc-biblioentry">
<div class="csl-left-margin"><strong>20. Training data-efficient image transformers <span>&amp;</span> distillation through attention</strong>. </div><div class="csl-right-inline">Touvron, H., Cord, M., Douze, M., Massa, F., Sablayrolles, A. &amp; Jégou, H., 2020. </div>
</div>
<div id="ref-depth" class="csl-entry" role="doc-biblioentry">
<div class="csl-left-margin"><strong>21. Deep networks with stochastic depth</strong>. </div><div class="csl-right-inline">Huang, G., Sun, Y., Liu, Z., Sedra, D. &amp; Weinberger, K. Q., 2016. </div>
</div>
<div id="ref-randaugment" class="csl-entry" role="doc-biblioentry">
<div class="csl-left-margin"><strong>22. RandAugment: Practical automated data augmentation with a reduced search space</strong>. </div><div class="csl-right-inline">Cubuk, E. D., Zoph, B., Shlens, J. &amp; Le, Q., 2020. </div>
</div>
<div id="ref-mixup" class="csl-entry" role="doc-biblioentry">
<div class="csl-left-margin"><strong>23. Mixup: Beyond empirical risk minimization</strong>. </div><div class="csl-right-inline">Zhang, H., Cissé, M., Dauphin, Y. N. &amp; Lopez-Paz, D., 2018. </div>
</div>
<div id="ref-cutmix" class="csl-entry" role="doc-biblioentry">
<div class="csl-left-margin"><strong>24. CutMix: Regularization strategy to train strong classifiers with localizable features</strong>. </div><div class="csl-right-inline">Yun, S., Han, D., Chun, S., Oh, S. J., Yoo, Y. &amp; Choe, J., 2019. </div>
</div>
<div id="ref-randerasing" class="csl-entry" role="doc-biblioentry">
<div class="csl-left-margin"><strong>25. Random erasing data augmentation</strong>. </div><div class="csl-right-inline">Zhong, Z., Zheng, L., Kang, G., Li, S. &amp; Yang, Y., 2020. </div>
</div>
<div id="ref-dropout" class="csl-entry" role="doc-biblioentry">
<div class="csl-left-margin"><strong>26. Dropout: A simple way to prevent neural networks from overfitting</strong>. </div><div class="csl-right-inline">Srivastava, N., Hinton, G. E., Krizhevsky, A., Sutskever, I. &amp; Salakhutdinov, R., 2014. </div>
</div>
<div id="ref-efficientnetv1" class="csl-entry" role="doc-biblioentry">
<div class="csl-left-margin"><strong>27. EfficientNet: Rethinking model scaling for convolutional neural networks</strong>. </div><div class="csl-right-inline">Tan, M. &amp; Le, Q. V., 2019. </div>
</div>
<div id="ref-botnet" class="csl-entry" role="doc-biblioentry">
<div class="csl-left-margin"><strong>28. Bottleneck transformers for visual recognition</strong>. </div><div class="csl-right-inline">Srinivas, A., Lin, T.-Y., Parmar, N., Shlens, J., Abbeel, P. &amp; Vaswani, A., 2021. </div>
</div>
<div id="ref-se" class="csl-entry" role="doc-biblioentry">
<div class="csl-left-margin"><strong>29. Squeeze-and-excitation networks</strong>. </div><div class="csl-right-inline">Hu, J., Shen, L., Albanie, S., Sun, G. &amp; Wu, E., 2020. </div>
</div>
<div id="ref-silu" class="csl-entry" role="doc-biblioentry">
<div class="csl-left-margin"><strong>30. Sigmoid-weighted linear units for neural network function approximation in reinforcement learning</strong>. </div><div class="csl-right-inline">Elfwing, S., Uchibe, E. &amp; Doya, K., 2018. </div>
</div>
<div id="ref-conditional" class="csl-entry" role="doc-biblioentry">
<div class="csl-left-margin"><strong>31. Conditional positional encodings for vision transformers</strong>. </div><div class="csl-right-inline">Chu, X., Tian, Z., Zhang, B., <em>et al.</em>, 2021. </div>
</div>
<div id="ref-tnt" class="csl-entry" role="doc-biblioentry">
<div class="csl-left-margin"><strong>32. Transformer in transformer</strong>. </div><div class="csl-right-inline">Han, K., Xiao, A., Wu, E., Guo, J., Xu, C. &amp; Wang, Y., 2021. </div>
</div>
<div id="ref-deepvit" class="csl-entry" role="doc-biblioentry">
<div class="csl-left-margin"><strong>33. DeepViT: Towards deeper vision transformer</strong>. </div><div class="csl-right-inline">Zhou, D., Kang, B., Jin, X., <em>et al.</em>, 2021. </div>
</div>
<div id="ref-lazy" class="csl-entry" role="doc-biblioentry">
<div class="csl-left-margin"><strong>34. LazyFormer: Self attention with lazy update</strong>. </div><div class="csl-right-inline">Ying, C., Ke, G., He, D. &amp; Liu, T.-Y., 2021. </div>
</div>
<div id="ref-cait" class="csl-entry" role="doc-biblioentry">
<div class="csl-left-margin"><strong>35. Going deeper with image transformers</strong>. </div><div class="csl-right-inline">Touvron, H., Cord, M., Sablayrolles, A., Synnaeve, G. &amp; Jégou, H., 2021. </div>
</div>
<div id="ref-fixup" class="csl-entry" role="doc-biblioentry">
<div class="csl-left-margin"><strong>36. Fixup initialization: Residual learning without normalization</strong>. </div><div class="csl-right-inline">Zhang, H., Dauphin, Y. N. &amp; Ma, T., 2019. </div>
</div>
<div id="ref-skipinit" class="csl-entry" role="doc-biblioentry">
<div class="csl-left-margin"><strong>37. Batch normalization biases residual blocks towards the identity function in deep networks</strong>. </div><div class="csl-right-inline">De, S. &amp; Smith, S. L., 2020. </div>
</div>
<div id="ref-talking" class="csl-entry" role="doc-biblioentry">
<div class="csl-left-margin"><strong>38. Talking-heads attention</strong>. </div><div class="csl-right-inline">Shazeer, N., Lan, Z., Cheng, Y., Ding, N. &amp; Hou, L., 2020. </div>
</div>
<div id="ref-nfnet" class="csl-entry" role="doc-biblioentry">
<div class="csl-left-margin"><strong>39. High-performance large-scale image recognition without normalization</strong>. </div><div class="csl-right-inline">Brock, A., De, S., Smith, S. L. &amp; Simonyan, K., 2021. </div>
</div>
<div id="ref-convit" class="csl-entry" role="doc-biblioentry">
<div class="csl-left-margin"><strong>40. ConViT: Improving vision transformers with soft convolutional inductive biases</strong>. </div><div class="csl-right-inline">d’Ascoli, S., Touvron, H., Leavitt, M. L., Morcos, A. S., Biroli, G. &amp; Sagun, L., 2021. </div>
</div>
<div id="ref-relationship" class="csl-entry" role="doc-biblioentry">
<div class="csl-left-margin"><strong>41. On the relationship between self-attention and convolutional layers</strong>. </div><div class="csl-right-inline">Cordonnier, J.-B., Loukas, A. &amp; Jaggi, M., 2020. </div>
</div>
<div id="ref-ceit" class="csl-entry" role="doc-biblioentry">
<div class="csl-left-margin"><strong>42. Incorporating convolution designs into visual transformers</strong>. </div><div class="csl-right-inline">Yuan, K., Guo, S., Liu, Z., Zhou, A., Yu, F. &amp; Wu, W., 2021. </div>
</div>
<div id="ref-localvit" class="csl-entry" role="doc-biblioentry">
<div class="csl-left-margin"><strong>43. LocalViT: Bringing locality to vision transformers</strong>. </div><div class="csl-right-inline">Li, Y., Zhang, K., Cao, J., Timofte, R. &amp; Gool, L. V., 2021. </div>
</div>
<div id="ref-t2t" class="csl-entry" role="doc-biblioentry">
<div class="csl-left-margin"><strong>44. Tokens-to-token ViT: Training vision transformers from scratch on ImageNet</strong>. </div><div class="csl-right-inline">Yuan, L., Chen, Y., Wang, T., <em>et al.</em>, 2021. </div>
</div>
<div id="ref-mobile" class="csl-entry" role="doc-biblioentry">
<div class="csl-left-margin"><strong>45. MobileNetV2: Inverted residuals and linear bottlenecks</strong>. </div><div class="csl-right-inline">Sandler, M., Howard, A. G., Zhu, M., Zhmoginov, A. &amp; Chen, L.-C., 2018. </div>
</div>
<div id="ref-pvt" class="csl-entry" role="doc-biblioentry">
<div class="csl-left-margin"><strong>46. Pyramid vision transformer: <span>A</span> versatile backbone for dense prediction without convolutions</strong>. </div><div class="csl-right-inline">Wang, W., Xie, E., Li, X., <em>et al.</em>, 2021. </div>
</div>
<div id="ref-hvt" class="csl-entry" role="doc-biblioentry">
<div class="csl-left-margin"><strong>47. Scalable visual transformers with hierarchical pooling</strong>. </div><div class="csl-right-inline">Pan, Z., Zhuang, B., Liu, J., He, H. &amp; Cai, J., 2021. </div>
</div>
<div id="ref-swin" class="csl-entry" role="doc-biblioentry">
<div class="csl-left-margin"><strong>48. Swin transformer: Hierarchical vision transformer using shifted windows</strong>. </div><div class="csl-right-inline">Liu, Z., Lin, Y., Cao, Y., <em>et al.</em>, 2021. </div>
</div>
<div id="ref-pit" class="csl-entry" role="doc-biblioentry">
<div class="csl-left-margin"><strong>49. Rethinking spatial dimensions of vision transformers</strong>. </div><div class="csl-right-inline">Heo, B., Yun, S., Han, D., Chun, S., Choe, J. &amp; Oh, S. J., 2021. </div>
</div>
<div id="ref-levit" class="csl-entry" role="doc-biblioentry">
<div class="csl-left-margin"><strong>50. LeViT: A vision transformer in ConvNet’s clothing for faster inference</strong>. </div><div class="csl-right-inline">Graham, B., El-Nouby, A., Touvron, H., <em>et al.</em>, 2021. </div>
</div>
<div id="ref-gelu" class="csl-entry" role="doc-biblioentry">
<div class="csl-left-margin"><strong>51. Bridging nonlinearities and stochastic regularizers with gaussian error linear units</strong>. </div><div class="csl-right-inline">Hendrycks, D. &amp; Gimpel, K., 2016. </div>
</div>
<div id="ref-crossvit" class="csl-entry" role="doc-biblioentry">
<div class="csl-left-margin"><strong>52. CrossViT: Cross-attention multi-scale vision transformer for image classification</strong>. </div><div class="csl-right-inline">Chen, C.-F., Fan, Q. &amp; Panda, R., 2021. </div>
</div>
<div id="ref-cvt" class="csl-entry" role="doc-biblioentry">
<div class="csl-left-margin"><strong>53. CvT: Introducing convolutions to vision transformers</strong>. </div><div class="csl-right-inline">Wu, H., Xiao, B., Codella, N., <em>et al.</em>, 2021. </div>
</div>
<div id="ref-depthwise" class="csl-entry" role="doc-biblioentry">
<div class="csl-left-margin"><strong>54. Xception: Deep learning with depthwise separable convolutions</strong>. </div><div class="csl-right-inline">Chollet, F., 2017. </div>
</div>
<div id="ref-vil" class="csl-entry" role="doc-biblioentry">
<div class="csl-left-margin"><strong>55. Multi-scale vision longformer: <span>A</span> new vision transformer for high-resolution image encoding</strong>. </div><div class="csl-right-inline">Zhang, P., Dai, X., Yang, J., <em>et al.</em>, 2021. </div>
</div>
<div id="ref-long" class="csl-entry" role="doc-biblioentry">
<div class="csl-left-margin"><strong>56. Longformer: The long-document transformer</strong>. </div><div class="csl-right-inline">Beltagy, I., Peters, M. E. &amp; Cohan, A., 2020. </div>
</div>
<div id="ref-halonet" class="csl-entry" role="doc-biblioentry">
<div class="csl-left-margin"><strong>57. Scaling local self-attention for parameter efficient visual backbones</strong>. </div><div class="csl-right-inline">Vaswani, A., Ramachandran, P., Srinivas, A., Parmar, N., Hechtman, B. A. &amp; Shlens, J., 2021. </div>
</div>
<div id="ref-sovit" class="csl-entry" role="doc-biblioentry">
<div class="csl-left-margin"><strong>58. So-ViT: Mind visual tokens for vision transformer</strong>. </div><div class="csl-right-inline">Xie, J., Zeng, R., Wang, Q., Zhou, Z. &amp; Li, P., 2021. </div>
</div>
<div id="ref-canziani" class="csl-entry" role="doc-biblioentry">
<div class="csl-left-margin"><strong>59. An analysis of deep neural network models for practical applications</strong>. </div><div class="csl-right-inline">Canziani, A., Paszke, A. &amp; Culurciello, E., 2016. </div>
</div>
<div id="ref-resnetrs" class="csl-entry" role="doc-biblioentry">
<div class="csl-left-margin"><strong>60. Revisiting ResNets: Improved training and scaling strategies</strong>. </div><div class="csl-right-inline">Bello, I., Fedus, W., Du, X., <em>et al.</em>, 2021. </div>
</div>
<div id="ref-robustness" class="csl-entry" role="doc-biblioentry">
<div class="csl-left-margin"><strong>61. Understanding robustness of transformers for image classification</strong>. </div><div class="csl-right-inline">Bhojanapalli, S., Chakrabarti, A., Glasner, D., Li, D., Unterthiner, T. &amp; Veit, A., 2021. </div>
</div>
<div id="ref-gsa" class="csl-entry" role="doc-biblioentry">
<div class="csl-left-margin"><strong>62. Group equivariant stand-alone self-attention for vision</strong>. </div><div class="csl-right-inline">Romero, D. W. &amp; Cordonnier, J.-B., 2021. </div>
</div>
<div id="ref-igpt" class="csl-entry" role="doc-biblioentry">
<div class="csl-left-margin"><strong>63. Generative pretraining from pixels</strong>. </div><div class="csl-right-inline">Chen, M., Radford, A., Child, R., <em>et al.</em>, 2020. </div>
</div>
<div id="ref-random" class="csl-entry" role="doc-biblioentry">
<div class="csl-left-margin"><strong>64. An empirical study of training self-supervised vision transformers</strong>. </div><div class="csl-right-inline">Chen, X., Xie, S. &amp; He, K., 2021. </div>
</div>
<div id="ref-sit" class="csl-entry" role="doc-biblioentry">
<div class="csl-left-margin"><strong>65. SiT: Self-supervised vIsion transformer</strong>. </div><div class="csl-right-inline">Ahmed, S. A. A., Awais, M. &amp; Kittler, J., 2021. </div>
</div>
<div id="ref-transunet" class="csl-entry" role="doc-biblioentry">
<div class="csl-left-margin"><strong>66. TransUNet: Transformers make strong encoders for medical image segmentation</strong>. </div><div class="csl-right-inline">Chen, J., Lu, Y., Yu, Q., <em>et al.</em>, 2021. </div>
</div>
<div id="ref-medt" class="csl-entry" role="doc-biblioentry">
<div class="csl-left-margin"><strong>67. Medical transformer: Gated axial-attention for medical image segmentation</strong>. </div><div class="csl-right-inline">Valanarasu, J. M. J., Oza, P., Hacihaliloglu, I. &amp; Patel, V. M., 2021. </div>
</div>
<div id="ref-unetr" class="csl-entry" role="doc-biblioentry">
<div class="csl-left-margin"><strong>68. <span>UNETR:</span> Transformers for 3D medical image segmentation</strong>. </div><div class="csl-right-inline">Hatamizadeh, A., Yang, D., Roth, H. &amp; Xu, D., 2021. </div>
</div>
<div id="ref-vtn" class="csl-entry" role="doc-biblioentry">
<div class="csl-left-margin"><strong>69. Video transformer network</strong>. </div><div class="csl-right-inline">Neimark, D., Bar, O., Zohar, M. &amp; Asselmann, D., 2021. </div>
</div>
<div id="ref-slowfast" class="csl-entry" role="doc-biblioentry">
<div class="csl-left-margin"><strong>70. SlowFast networks for video recognition</strong>. </div><div class="csl-right-inline">Feichtenhofer, C., Fan, H., Malik, J. &amp; He, K., 2019. </div>
</div>
<div id="ref-timesformer" class="csl-entry" role="doc-biblioentry">
<div class="csl-left-margin"><strong>71. Is space-time attention all you need for video understanding?</strong> </div><div class="csl-right-inline">Bertasius, G., Wang, H. &amp; Torresani, L., 2021. </div>
</div>
<div id="ref-kinetics" class="csl-entry" role="doc-biblioentry">
<div class="csl-left-margin"><strong>72. The kinetics human action video dataset</strong>. </div><div class="csl-right-inline">Kay, W., Carreira, J., Simonyan, K., <em>et al.</em>, 2017. </div>
</div>
<div id="ref-videoworth" class="csl-entry" role="doc-biblioentry">
<div class="csl-left-margin"><strong>73. An image is worth 16x16 words, what is a video worth?</strong> </div><div class="csl-right-inline">Sharir, G., Noy, A. &amp; Zelnik-Manor, L., 2021. </div>
</div>
<div id="ref-x3d" class="csl-entry" role="doc-biblioentry">
<div class="csl-left-margin"><strong>74. <span>X3D:</span> Expanding architectures for efficient video recognition</strong>. </div><div class="csl-right-inline">Feichtenhofer, C., 2020. </div>
</div>
<div id="ref-vivit" class="csl-entry" role="doc-biblioentry">
<div class="csl-left-margin"><strong>75. ViViT: <span>A</span> video vision transformer</strong>. </div><div class="csl-right-inline">Arnab, A., Dehghani, M., Heigold, G., Sun, C., Lucic, M. &amp; Schmid, C., 2021. </div>
</div>
<div id="ref-mvit" class="csl-entry" role="doc-biblioentry">
<div class="csl-left-margin"><strong>76. Multiscale vision transformers</strong>. </div><div class="csl-right-inline">Fan, H., Xiong, B., Mangalam, K., <em>et al.</em>, 2021. </div>
</div>
<div id="ref-efficientnetv2" class="csl-entry" role="doc-biblioentry">
<div class="csl-left-margin"><strong>77. EfficientNetV2: Smaller models and faster training</strong>. </div><div class="csl-right-inline">Tan, M. &amp; Le, Q. V., 2021. </div>
</div>
<div id="ref-ast" class="csl-entry" role="doc-biblioentry">
<div class="csl-left-margin"><strong>78. <span>AST:</span> Audio spectrogram transformer</strong>. </div><div class="csl-right-inline">Gong, Y., Chung, Y.-A. &amp; Glass, J. R., 2021. </div>
</div>
<div id="ref-keyword" class="csl-entry" role="doc-biblioentry">
<div class="csl-left-margin"><strong>79. Keyword transformer: <span>A</span> self-attention model for keyword spotting</strong>. </div><div class="csl-right-inline">Berg, A., O’Connor, M. &amp; Cruz, M. T., 2021. </div>
</div>
<div id="ref-vilt" class="csl-entry" role="doc-biblioentry">
<div class="csl-left-margin"><strong>80. ViLT: Vision-and-language transformer without convolution or region supervision</strong>. </div><div class="csl-right-inline">Kim, W., Son, B. &amp; Kim, I., 2021. </div>
</div>
<div id="ref-vatt" class="csl-entry" role="doc-biblioentry">
<div class="csl-left-margin"><strong>81. <span>VATT:</span> Transformers for multimodal self-supervised learning from raw video, audio and text</strong>. </div><div class="csl-right-inline">Akbari, H., Yuan, L., Qian, R., <em>et al.</em>, 2021. </div>
</div>
<div id="ref-perceiver" class="csl-entry" role="doc-biblioentry">
<div class="csl-left-margin"><strong>82. Perceiver: General perception with iterative attention</strong>. </div><div class="csl-right-inline">Jaegle, A., Gimeno, F., Brock, A., Zisserman, A., Vinyals, O. &amp; Carreira, J., 2021. </div>
</div>
</div>
<!--radix_placeholder_article_footer-->
<!--/radix_placeholder_article_footer-->
</div>

<div class="d-appendix">
</div>

<!-- dynamically load mathjax for compatibility with self-contained -->
<script>
  (function () {
    var script = document.createElement("script");
    script.type = "text/javascript";
    script.src  = "https://mathjax.rstudio.com/latest/MathJax.js?config=TeX-AMS-MML_HTMLorMML";
    document.getElementsByTagName("head")[0].appendChild(script);
  })();
</script>

<!--radix_placeholder_site_after_body-->
<!--/radix_placeholder_site_after_body-->
<!--radix_placeholder_appendices-->
<div class="appendix-bottom">
  <h3 id="references">References</h3>
  <div id="references-listing"></div>
  <h3 id="reuse">Reuse</h3>
  <p>Text and figures are licensed under Creative Commons Attribution <a rel="license" href="https://creativecommons.org/licenses/by/4.0/">CC BY 4.0</a>. The figures that have been reused from other sources don't fall under this license and can be recognized by a note in their caption: "Figure from ...".</p>
  <h3 id="citation">Citation</h3>
  <p>For attribution, please cite this work as</p>
  <pre class="citation-appendix short">Zanichelli (2021, April 28). IAML Distill Blog: Transformers in Vision. Retrieved from https://iaml-it.github.io/distill/posts/2021-04-28-transformers-in-vision/</pre>
  <p>BibTeX citation</p>
  <pre class="citation-appendix long">@misc{zanichelli2021transformers,
  author = {Zanichelli, Niccolò},
  title = {IAML Distill Blog: Transformers in Vision},
  url = {https://iaml-it.github.io/distill/posts/2021-04-28-transformers-in-vision/},
  year = {2021}
}</pre>
</div>
<!--/radix_placeholder_appendices-->
<!--radix_placeholder_navigation_after_body-->
<!--/radix_placeholder_navigation_after_body-->

</body>

</html>
